{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import pyro\n",
    "import numpy as np\n",
    "import pyro.optim as optim\n",
    "import pyro.distributions as dist\n",
    "from pyro.infer import SVI, Trace_ELBO, TraceGraph_ELBO\n",
    "from os import walk\n",
    "import os\n",
    "import os.path\n",
    "import cv2\n",
    "import glob\n",
    "import imutils\n",
    "import uuid\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shutil import move\n",
    "\n",
    "sampleFolder = \"generated_captcha_images\"\n",
    "testFolder = \"test_images\"\n",
    "_, _, filenames = next(walk(sampleFolder))\n",
    "'''\n",
    "# moving 10% train data as test data\n",
    "test_files = random.sample(filenames, int(len(filenames)* 0.1))\n",
    "for f in test_files:\n",
    "    s_p = os.path.join(sampleFolder, f)\n",
    "    d_p = os.path.join(testFolder, f)\n",
    "    \n",
    "    move(s_p, d_p)\n",
    "'''\n",
    "_, _, test_filenames = next(walk(testFolder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Promoted by https://medium.com/@ageitgey/how-to-break-a-captcha-system-in-15-minutes-with-machine-learning-dbebb035a710\n",
    "def split_image(img, gd_truth_label, TRAIN=True):\n",
    "    # Add some extra padding around the image\n",
    "    gray = cv2.copyMakeBorder(img, 8, 8, 8, 8, cv2.BORDER_REPLICATE)\n",
    "\n",
    "    # threshold the image (convert it to pure black and white)\n",
    "    thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV | cv2.THRESH_OTSU)[1]\n",
    "\n",
    "    # find the contours (continuous blobs of pixels) the image\n",
    "    contours = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Hack for compatibility with different OpenCV versions\n",
    "    contours = contours[0] #if imutils.is_cv2() else contours[1]\n",
    "\n",
    "    letter_image_regions = []\n",
    "\n",
    "    # Now we can loop through each of the four contours and extract the letter\n",
    "    # inside of each one\n",
    "    for contour in contours:\n",
    "        # Get the rectangle that contains the contour\n",
    "        (x, y, w, h) = cv2.boundingRect(contour)\n",
    "\n",
    "        # Compare the width and height of the contour to detect letters that\n",
    "        # are conjoined into one chunk\n",
    "        if w / h > 1.25:\n",
    "            # This contour is too wide to be a single letter!\n",
    "            # Split it in half into two letter regions!\n",
    "            half_width = int(w / 2)\n",
    "            letter_image_regions.append((x, y, half_width, h))\n",
    "            letter_image_regions.append((x + half_width, y, half_width, h))\n",
    "        else:\n",
    "            # This is a normal letter by itself\n",
    "            letter_image_regions.append((x, y, w, h))\n",
    "    \n",
    "    if ((len(letter_image_regions) != 4) and TRAIN):\n",
    "        return None, None\n",
    "\n",
    "    # Sort the detected letter images based on the x coordinate to make sure\n",
    "    # we are processing them from left-to-right so we match the right image\n",
    "    # with the right letter\n",
    "    letter_image_regions = sorted(letter_image_regions, key=lambda x: x[0])\n",
    "    char_images = []\n",
    "    labels = []\n",
    "    # Save out each letter as a single image\n",
    "    for letter_bounding_box, letter_text in zip(letter_image_regions, gd_truth_label):\n",
    "        # Grab the coordinates of the letter in the image\n",
    "        x, y, w, h = letter_bounding_box\n",
    "\n",
    "        # Extract the letter from the original image with a 2-pixel margin around the edge\n",
    "        letter_image = gray[y - 2:y + h + 2, x - 2:x + w + 2]\n",
    "        char_images.append(letter_image)\n",
    "        labels.append(letter_text)\n",
    "    return char_images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Promoted by https://medium.com/@ageitgey/how-to-break-a-captcha-system-in-15-minutes-with-machine-learning-dbebb035a710\n",
    "def resize_to_fit(image, width, height):\n",
    "    \"\"\"\n",
    "    A helper function to resize an image to fit within a given size\n",
    "    :param image: image to resize\n",
    "    :param width: desired width in pixels\n",
    "    :param height: desired height in pixels\n",
    "    :return: the resized image\n",
    "    \"\"\"\n",
    "\n",
    "    # grab the dimensions of the image, then initialize\n",
    "    # the padding values\n",
    "    (h, w) = image.shape[:2]\n",
    "\n",
    "    # if the width is greater than the height then resize along\n",
    "    # the width\n",
    "    if w > h:\n",
    "        image = imutils.resize(image, width=width)\n",
    "\n",
    "    # otherwise, the height is greater than the width so resize\n",
    "    # along the height\n",
    "    else:\n",
    "        image = imutils.resize(image, height=height)\n",
    "\n",
    "    # determine the padding values for the width and height to\n",
    "    # obtain the target dimensions\n",
    "    padW = int((width - image.shape[1]) / 2.0)\n",
    "    padH = int((height - image.shape[0]) / 2.0)\n",
    "\n",
    "    # pad the image then apply one more resizing to handle any\n",
    "    # rounding issues\n",
    "    image = cv2.copyMakeBorder(image, padH, padH, padW, padW,\n",
    "        cv2.BORDER_REPLICATE)\n",
    "    image = cv2.resize(image, (width, height))\n",
    "\n",
    "    # return the pre-processed image\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9955 35\n",
      "torch.Size([1, 1, 24, 72])\n",
      "torch.FloatTensor\n",
      "GLOBAL_MEAN is 227.96146786697545\n",
      "KVP in data processed: 9686\n",
      "Optimizing...\n",
      "=================================\n",
      "OrderedDict([('neural_net.0.weight', tensor([[-0.0207,  0.0099, -0.0137,  ...,  0.0236, -0.0178, -0.0071],\n",
      "        [ 0.0237, -0.0227,  0.0141,  ...,  0.0108, -0.0111,  0.0057],\n",
      "        [-0.0119, -0.0238,  0.0102,  ..., -0.0032,  0.0136, -0.0074],\n",
      "        ...,\n",
      "        [ 0.0217,  0.0011,  0.0093,  ..., -0.0215,  0.0186, -0.0240],\n",
      "        [-0.0212, -0.0178,  0.0123,  ..., -0.0022, -0.0103,  0.0164],\n",
      "        [-0.0156, -0.0098, -0.0173,  ...,  0.0122, -0.0222, -0.0190]])), ('neural_net.0.bias', tensor([ 0.0006, -0.0200, -0.0094,  ..., -0.0225,  0.0143, -0.0231])), ('neural_net.2.weight', tensor([[ 0.0053,  0.0062, -0.0010,  ...,  0.0142,  0.0074,  0.0121],\n",
      "        [-0.0033, -0.0168, -0.0086,  ..., -0.0010,  0.0043, -0.0018],\n",
      "        [ 0.0053, -0.0167,  0.0029,  ...,  0.0019,  0.0165,  0.0059],\n",
      "        ...,\n",
      "        [-0.0010,  0.0061,  0.0008,  ..., -0.0149, -0.0049,  0.0038],\n",
      "        [-0.0023,  0.0023,  0.0062,  ..., -0.0103, -0.0107, -0.0160],\n",
      "        [ 0.0169, -0.0160,  0.0136,  ..., -0.0100,  0.0099, -0.0165]])), ('neural_net.2.bias', tensor([-0.0082, -0.0131, -0.0010, -0.0076, -0.0167, -0.0065,  0.0141, -0.0031,\n",
      "        -0.0109, -0.0105,  0.0010,  0.0124,  0.0060,  0.0019, -0.0155,  0.0035,\n",
      "        -0.0150,  0.0015,  0.0151, -0.0061,  0.0112, -0.0149, -0.0077, -0.0149,\n",
      "        -0.0025,  0.0110,  0.0158,  0.0014,  0.0141, -0.0032, -0.0093, -0.0166,\n",
      "         0.0155,  0.0064,  0.0121, -0.0031,  0.0055,  0.0125, -0.0170, -0.0028,\n",
      "         0.0153, -0.0156, -0.0144,  0.0032,  0.0030, -0.0167, -0.0032, -0.0114,\n",
      "        -0.0109,  0.0024,  0.0029,  0.0106, -0.0113,  0.0088,  0.0078, -0.0002,\n",
      "        -0.0001,  0.0066, -0.0093,  0.0137,  0.0021, -0.0155, -0.0056,  0.0097,\n",
      "         0.0169, -0.0045,  0.0131, -0.0062, -0.0007, -0.0002,  0.0009, -0.0140,\n",
      "         0.0120, -0.0018,  0.0164,  0.0153, -0.0024, -0.0044,  0.0101, -0.0006,\n",
      "        -0.0072,  0.0068, -0.0149,  0.0049,  0.0137, -0.0095, -0.0103,  0.0099,\n",
      "         0.0008, -0.0046, -0.0112,  0.0169, -0.0129,  0.0032, -0.0125,  0.0022,\n",
      "         0.0054, -0.0067,  0.0034, -0.0132,  0.0066,  0.0113, -0.0013, -0.0134,\n",
      "        -0.0142,  0.0130, -0.0136,  0.0135, -0.0143,  0.0120, -0.0089, -0.0067,\n",
      "        -0.0082,  0.0038,  0.0107,  0.0079,  0.0149,  0.0051, -0.0048, -0.0114,\n",
      "        -0.0108, -0.0075, -0.0132, -0.0024, -0.0090, -0.0081,  0.0132, -0.0010])), ('neural_net.4.weight', tensor([[-0.0117, -0.0495,  0.0768,  ..., -0.0321,  0.0723, -0.0141],\n",
      "        [ 0.0280, -0.0311,  0.0150,  ..., -0.0141, -0.0769,  0.0753],\n",
      "        [ 0.0247, -0.0396,  0.0400,  ..., -0.0577,  0.0247,  0.0412],\n",
      "        ...,\n",
      "        [-0.0606, -0.0777, -0.0863,  ...,  0.0802, -0.0504,  0.0314],\n",
      "        [ 0.0067, -0.0233,  0.0043,  ...,  0.0228,  0.0830,  0.0819],\n",
      "        [ 0.0799, -0.0334, -0.0527,  ..., -0.0192,  0.0757, -0.0049]])), ('neural_net.4.bias', tensor([ 0.0547, -0.0334, -0.0452, -0.0633, -0.0051,  0.0773, -0.0266,  0.0089,\n",
      "         0.0504,  0.0119]))])\n",
      "+++++++++++++++++++++++++++++++++\n",
      "OrderedDict([('neural_net.0.weight', tensor([[ 0.0300,  0.0118, -0.0234,  ...,  0.0491,  0.0055,  0.0172],\n",
      "        [ 0.0244, -0.0491, -0.0088,  ...,  0.0126,  0.0207, -0.0153],\n",
      "        [ 0.0261,  0.0437, -0.0447,  ..., -0.0181, -0.0320, -0.0487],\n",
      "        ...,\n",
      "        [ 0.0221,  0.0359, -0.0242,  ...,  0.0133, -0.0426, -0.0433],\n",
      "        [ 0.0344,  0.0064, -0.0412,  ..., -0.0123,  0.0346, -0.0026],\n",
      "        [-0.0264,  0.0044,  0.0168,  ...,  0.0167, -0.0411, -0.0457]])), ('neural_net.0.bias', tensor([ 1.1061e-02,  3.8720e-03, -4.7112e-02, -4.5548e-02,  3.9115e-02,\n",
      "         2.4359e-02,  1.0684e-02, -3.5915e-02,  2.7317e-02, -1.2059e-02,\n",
      "         1.5699e-02,  3.4849e-02, -1.7847e-02, -1.2548e-02,  2.9066e-02,\n",
      "         4.2945e-02, -4.2294e-02,  2.3340e-02, -2.0091e-02,  1.6778e-02,\n",
      "        -1.7084e-02, -6.0201e-03,  2.6721e-02, -2.9464e-02,  7.3165e-03,\n",
      "        -2.2680e-02,  4.2206e-02, -3.7370e-02,  1.9917e-02, -2.1566e-02,\n",
      "         4.2183e-02, -2.8002e-02,  2.0796e-02, -1.9697e-02, -3.9732e-02,\n",
      "        -2.0784e-02, -2.9875e-02, -3.8290e-02, -1.8087e-02,  8.0548e-03,\n",
      "        -4.5750e-02, -3.6766e-02,  3.0983e-02,  2.4398e-02,  4.5440e-02,\n",
      "        -2.6009e-02, -3.0577e-02, -4.6830e-03,  6.2271e-04, -6.1774e-03,\n",
      "         2.8444e-02,  3.1986e-02,  1.4618e-02,  4.2144e-02,  3.1361e-02,\n",
      "         1.6304e-02, -3.7198e-02, -4.7314e-02, -9.4005e-03, -2.2358e-02,\n",
      "         1.6049e-02, -2.3863e-02, -2.3332e-02, -4.0415e-03, -3.9242e-02,\n",
      "        -4.4553e-04, -4.4611e-02, -1.9907e-02,  5.6548e-03,  3.1399e-02,\n",
      "         3.2938e-02,  4.3608e-02, -1.4737e-02,  2.6348e-04,  3.9897e-02,\n",
      "         1.3274e-02, -7.7038e-03, -3.4665e-02, -2.2125e-02,  3.9118e-02,\n",
      "         3.7904e-02, -4.8246e-03, -3.4064e-03, -1.2740e-02, -3.9803e-02,\n",
      "         2.3103e-02, -9.9103e-03,  1.0267e-02, -2.6825e-02,  1.5584e-03,\n",
      "        -3.3159e-02, -1.8832e-02,  2.5692e-02, -2.6442e-02, -2.4869e-02,\n",
      "        -3.9733e-03,  5.4108e-03, -3.9771e-02, -1.0789e-02,  2.7796e-02,\n",
      "         3.3167e-02,  1.3762e-02,  4.8098e-02,  4.7983e-02, -1.5884e-02,\n",
      "         2.4714e-02, -4.4416e-02,  4.3697e-02,  2.3700e-02,  3.5229e-02,\n",
      "        -2.2044e-02, -1.2716e-02,  1.4302e-02,  9.8059e-03, -4.6725e-02,\n",
      "        -3.1743e-02, -4.5446e-02, -1.2965e-02, -9.2667e-03,  3.2884e-02,\n",
      "        -1.3219e-02, -4.7005e-03,  1.4594e-02, -2.3709e-02,  1.4685e-02,\n",
      "         4.9711e-02, -9.0326e-03,  2.7603e-03, -1.0102e-02,  1.6897e-02,\n",
      "         1.5810e-02,  2.7270e-03, -3.3186e-02, -4.6618e-02, -4.3314e-02,\n",
      "         2.6232e-02,  8.9827e-03, -4.0690e-02,  4.0559e-02,  2.4277e-02,\n",
      "        -6.0856e-03, -1.9566e-02, -5.2907e-03, -4.0674e-02,  1.4975e-02,\n",
      "        -8.3131e-03,  1.6475e-02,  1.8561e-02,  5.4160e-03, -2.2814e-02,\n",
      "        -2.7152e-02,  1.6013e-02, -4.1028e-02,  3.0543e-02,  4.6075e-02,\n",
      "         8.3315e-05, -2.7549e-02, -4.9070e-02, -2.2451e-02,  2.7261e-02,\n",
      "         1.0559e-02,  2.7855e-02, -2.4325e-02,  2.7014e-04,  3.9823e-02,\n",
      "         1.9469e-02,  2.4871e-02,  2.2058e-02, -3.6879e-02,  4.0815e-02,\n",
      "         3.3015e-02,  2.4142e-02, -3.0266e-02, -2.2254e-02,  1.0068e-02,\n",
      "         3.1732e-02,  2.4779e-02,  8.3267e-03, -2.8580e-02, -1.0863e-03,\n",
      "        -3.0786e-03, -3.9677e-02, -1.5157e-02, -3.5390e-02,  8.0407e-04,\n",
      "        -1.5532e-04,  3.4495e-02,  5.2087e-03, -8.0496e-03,  4.4494e-02,\n",
      "        -6.7061e-03,  3.8526e-02,  2.5465e-02, -2.6002e-02, -2.3854e-02,\n",
      "        -8.4088e-03, -5.6252e-03, -9.2301e-03,  1.0625e-02,  1.0172e-02,\n",
      "        -3.6533e-02, -3.3072e-02, -2.7309e-02, -4.6631e-02,  3.4789e-02,\n",
      "         4.7055e-02, -2.2926e-02, -1.8541e-02, -1.6434e-02,  1.9775e-02,\n",
      "         2.2140e-02,  1.7622e-02,  1.5736e-02, -6.8938e-03,  2.2334e-03,\n",
      "         3.7264e-02,  3.9526e-02,  2.5224e-02, -9.9040e-03, -2.4718e-02,\n",
      "         3.7246e-03, -5.1164e-03, -4.3180e-02,  2.2446e-02, -8.6385e-03,\n",
      "        -1.9385e-02,  2.6784e-02, -3.2739e-02, -1.0577e-02, -2.8241e-02,\n",
      "         4.2504e-02, -2.0331e-03,  4.5951e-02,  4.4969e-02,  5.4190e-03,\n",
      "         1.2550e-02,  4.6809e-02, -4.8370e-02, -2.5648e-02,  8.1954e-03,\n",
      "         2.5225e-02, -1.5901e-03,  1.2865e-02,  3.6678e-02, -3.5310e-02,\n",
      "        -2.2155e-02,  1.5285e-02, -3.1302e-02,  2.6981e-02,  2.3562e-02,\n",
      "        -4.3196e-02, -3.9768e-03, -2.2692e-02, -2.4214e-02, -3.3371e-02,\n",
      "        -5.7817e-03, -3.8680e-02, -2.5465e-02, -8.0994e-03,  3.3112e-02,\n",
      "         3.0001e-02, -2.9710e-02, -3.5089e-02,  1.5128e-02, -3.1262e-03,\n",
      "         5.9520e-03,  3.5386e-02, -4.6740e-03, -4.2232e-02, -5.3378e-03,\n",
      "         1.1907e-02, -4.1991e-02, -3.0325e-02, -2.9839e-02,  3.7174e-02,\n",
      "         3.4291e-02, -1.9534e-02, -3.6429e-02, -1.5199e-02, -3.7646e-02,\n",
      "         1.6612e-02,  3.0821e-02,  2.5936e-02,  1.7533e-02,  7.4717e-03,\n",
      "        -3.0850e-02,  3.8989e-02, -3.6464e-02,  1.5215e-02,  4.6502e-02,\n",
      "         3.8833e-02,  4.5785e-02, -3.5088e-02,  1.8013e-02,  4.8654e-02,\n",
      "        -4.2814e-02,  7.8243e-03,  1.6110e-03, -3.2933e-02,  4.4556e-02,\n",
      "         2.3964e-03,  6.6400e-03,  3.4942e-02,  1.8970e-02,  8.5311e-03,\n",
      "        -4.6784e-02, -4.7365e-02,  4.9111e-02, -3.6339e-03,  1.6410e-02,\n",
      "        -3.6106e-02,  1.5857e-02, -4.4767e-02, -2.3960e-02, -6.8346e-03,\n",
      "         3.4991e-02,  3.0425e-04, -3.8061e-02, -7.1857e-03,  2.0041e-02,\n",
      "        -9.2156e-03, -1.2397e-02, -6.9294e-03,  4.1467e-02,  3.4419e-02,\n",
      "        -1.6804e-02,  5.0888e-03,  8.3733e-03,  4.1333e-02,  2.8457e-02,\n",
      "         6.0221e-04,  2.5564e-02,  8.8844e-04,  1.1101e-02,  5.0624e-03,\n",
      "        -1.8958e-02, -4.3959e-02,  4.5424e-04, -4.6224e-02,  3.3834e-02,\n",
      "         3.4079e-02,  4.4283e-02,  3.5406e-02,  3.6932e-02,  1.6665e-02,\n",
      "        -9.3032e-03,  1.4739e-02, -4.7208e-02,  2.0257e-02, -8.9058e-03,\n",
      "         4.7572e-03,  4.4681e-02,  2.2724e-02,  2.1541e-02,  6.1708e-03,\n",
      "        -7.7084e-03,  3.6847e-02,  1.5745e-02,  4.1886e-02,  1.5992e-02,\n",
      "         3.3412e-02, -3.0813e-02, -3.8937e-02, -4.7177e-03, -4.8014e-02,\n",
      "         4.2167e-02, -2.9533e-03, -1.1274e-02,  1.5946e-02,  6.5844e-03,\n",
      "         6.0405e-03,  4.1576e-02, -5.5946e-04,  3.2255e-02,  1.9228e-02,\n",
      "        -4.3025e-02, -6.3015e-03, -1.4189e-03,  2.0633e-02,  2.9591e-02,\n",
      "        -7.1713e-03,  3.5673e-02, -1.9976e-02,  3.1564e-02, -3.7450e-02,\n",
      "         9.1702e-03, -4.9550e-02,  3.0956e-03,  4.8376e-02,  2.6044e-02,\n",
      "        -7.6131e-03, -4.7726e-02,  4.5904e-02, -4.8664e-02, -3.3377e-02,\n",
      "         1.5523e-02,  4.7370e-02,  2.0716e-02,  1.7413e-02, -3.1198e-02,\n",
      "        -1.4619e-02, -3.7982e-02, -2.2436e-02, -4.8787e-02, -3.0379e-02,\n",
      "         2.3994e-02, -1.4392e-02, -2.1591e-02, -1.0737e-02,  3.2299e-02,\n",
      "         1.8437e-02, -8.8698e-03, -1.9353e-02,  3.0649e-04, -4.7145e-02,\n",
      "        -3.5216e-03,  9.2544e-03, -4.0353e-02, -2.0033e-03, -8.7881e-03,\n",
      "        -2.0896e-02, -4.3370e-02,  6.2355e-03, -1.6219e-02, -2.0568e-03,\n",
      "         6.9781e-03,  7.0080e-03,  1.9877e-02,  1.9010e-02, -4.1194e-02,\n",
      "         1.0486e-02,  2.1914e-03, -4.5107e-03,  3.4486e-02, -6.0811e-03,\n",
      "         8.3080e-03, -1.3092e-02, -2.3739e-02, -1.6597e-03,  6.1718e-03,\n",
      "         2.6061e-02, -3.8567e-02, -3.1856e-02,  2.5018e-02, -9.5387e-03,\n",
      "         1.1040e-02,  3.3774e-02,  4.3861e-02,  3.8320e-02, -3.8773e-02,\n",
      "         4.6874e-02,  4.5238e-02,  2.3432e-02,  3.9911e-02, -1.5843e-02,\n",
      "        -1.6282e-02, -1.5894e-02, -4.6440e-03, -2.1233e-02, -1.3593e-02,\n",
      "         2.1433e-02, -2.8368e-02,  9.0315e-03,  2.2319e-02,  1.0295e-02,\n",
      "         4.3383e-02,  6.6063e-03,  3.1404e-02,  1.1221e-02,  1.0643e-02,\n",
      "        -2.6621e-02,  2.4969e-02,  4.6800e-02,  1.1702e-02,  5.3638e-03,\n",
      "        -3.5293e-02, -1.1744e-02, -3.0293e-02, -3.4963e-02, -4.0315e-02,\n",
      "        -5.3337e-03,  7.1552e-03, -2.3167e-02,  4.3886e-03, -1.0472e-02,\n",
      "         1.3436e-02,  2.6627e-02, -1.2722e-02,  4.6512e-02, -2.8090e-02,\n",
      "        -1.5877e-02,  1.8929e-02, -3.1641e-02,  2.7022e-02,  4.5442e-02,\n",
      "         3.2255e-02, -4.1328e-02,  2.0364e-02, -3.4152e-02,  2.5370e-02,\n",
      "         4.8888e-02, -6.0997e-03,  2.8268e-02,  1.8241e-02,  3.4129e-02,\n",
      "         4.6517e-02,  6.5623e-03,  3.5699e-02,  4.2298e-02,  2.7631e-02,\n",
      "         2.5841e-02, -5.6542e-03, -9.6583e-03,  2.1434e-02, -5.5301e-03,\n",
      "         3.6750e-03,  3.8342e-02,  4.8242e-02, -1.8141e-02, -4.1024e-02,\n",
      "        -4.0571e-02, -3.9729e-02, -2.5404e-02,  1.8625e-02, -4.6184e-03,\n",
      "         3.7116e-02,  2.7879e-02, -1.0422e-02,  3.0773e-03, -8.0005e-03,\n",
      "        -2.1813e-02, -3.1977e-02,  1.8189e-02, -1.6879e-02,  1.6217e-02,\n",
      "         1.2786e-02,  2.5372e-02,  2.2881e-02,  1.9869e-02,  3.7403e-02,\n",
      "        -1.9059e-02,  7.8819e-03, -5.7852e-03, -2.5592e-02, -3.5255e-02,\n",
      "         4.6571e-02,  2.8229e-03, -2.5045e-02, -4.6248e-02,  2.9193e-03,\n",
      "         4.9460e-03, -3.8088e-03,  2.2409e-02, -1.9413e-02, -3.7376e-02,\n",
      "         4.8485e-02,  7.0409e-03,  4.2469e-02,  2.7533e-02, -3.9881e-02,\n",
      "        -2.4790e-02, -2.0737e-02, -3.6070e-02,  2.9796e-02, -1.5519e-02,\n",
      "        -4.3599e-02,  7.7021e-03, -1.9776e-02,  4.3751e-02, -2.9000e-02,\n",
      "        -3.8813e-02,  3.7486e-02,  2.7524e-02, -5.8983e-03, -2.3432e-02,\n",
      "         4.5749e-02, -1.4672e-02, -1.8528e-02, -2.6930e-02,  2.7746e-02,\n",
      "        -2.3604e-02,  3.4489e-02,  2.0136e-02, -4.3482e-02,  3.5061e-03,\n",
      "        -1.3871e-02,  3.8071e-02,  3.8404e-02,  2.8803e-02,  1.3476e-02,\n",
      "        -1.1029e-02, -1.8225e-02, -2.5057e-03,  4.4177e-02, -3.8934e-02,\n",
      "         9.5143e-03,  6.1765e-04,  4.7740e-02, -2.9900e-02,  2.8781e-02,\n",
      "         2.7706e-03, -2.7639e-02,  1.0185e-02, -3.7175e-02,  8.0023e-03,\n",
      "        -1.0259e-02,  3.3190e-02, -1.3853e-02,  2.6058e-02,  1.5849e-02,\n",
      "         1.7833e-02,  2.6363e-02,  7.6858e-04, -2.7740e-02, -2.0915e-02,\n",
      "        -3.1253e-02, -3.9849e-02, -3.1403e-02,  2.2033e-02,  1.6721e-02,\n",
      "        -8.7615e-03, -1.2020e-02, -3.8496e-02, -4.4580e-02,  1.5889e-02,\n",
      "        -1.4432e-02, -1.1884e-02, -1.7911e-02, -4.0338e-02, -2.1385e-02,\n",
      "        -1.5138e-02, -2.6253e-02,  3.9479e-02, -1.8689e-02,  2.7268e-02,\n",
      "        -3.6248e-03,  1.8118e-02, -2.6103e-02, -2.4800e-02, -1.9300e-02,\n",
      "         1.4784e-02,  3.0414e-02, -2.2604e-02,  2.4112e-02,  2.8602e-02,\n",
      "         3.6516e-03,  1.9037e-03,  4.6679e-02, -4.8316e-02,  3.7096e-02,\n",
      "        -3.2811e-02, -1.7213e-02,  4.7209e-02,  1.7617e-02,  1.4741e-02,\n",
      "        -2.2810e-02, -8.2551e-03, -1.6805e-02,  3.7154e-03,  1.5261e-02,\n",
      "         2.5472e-02, -2.8570e-02,  3.2031e-02,  3.7420e-02,  4.6240e-02,\n",
      "        -5.7981e-03, -1.1631e-02, -2.0747e-02, -4.3672e-02,  4.7060e-02,\n",
      "         1.3250e-02,  3.9320e-02,  3.2537e-04, -1.3843e-02,  2.7543e-02,\n",
      "         5.5364e-03,  2.1530e-02,  1.3334e-03,  6.8478e-04,  3.4394e-02,\n",
      "        -5.6849e-03, -3.6813e-03, -3.2600e-02, -2.3977e-02, -3.6578e-02,\n",
      "        -2.4773e-04,  3.0345e-02,  1.7047e-02,  4.2142e-02,  8.2055e-03,\n",
      "        -8.8504e-03,  1.7174e-02, -2.6023e-03,  3.8830e-02, -6.0580e-03,\n",
      "        -4.7424e-02,  4.6009e-02, -2.4438e-02, -4.7390e-02,  4.4815e-02,\n",
      "         2.9679e-02,  1.9735e-02,  7.2148e-03, -2.3359e-02,  3.4358e-02,\n",
      "         4.1071e-02, -1.6914e-02, -1.2411e-02, -2.5708e-02, -2.5788e-02,\n",
      "         1.2985e-02, -4.1770e-02, -2.5053e-02, -3.2311e-02,  8.0385e-03,\n",
      "         2.5624e-02,  4.5532e-02,  2.1619e-02,  3.2103e-02, -1.1225e-02,\n",
      "         1.1576e-02, -3.3426e-02,  3.9013e-02, -4.8156e-02, -3.6764e-02,\n",
      "         3.6887e-02, -4.8344e-02,  4.3062e-02,  4.4600e-03, -1.5408e-02,\n",
      "        -1.7020e-02, -7.9599e-03, -3.4758e-02, -1.1601e-02, -1.5966e-02,\n",
      "         3.9113e-02,  2.5943e-02, -3.8097e-02, -3.9269e-02,  3.2223e-02,\n",
      "        -1.3716e-02, -2.3818e-02, -1.6482e-02, -8.0712e-03,  6.1746e-03,\n",
      "         9.6962e-03, -7.9078e-04, -2.4006e-02, -3.4756e-02,  1.7271e-02,\n",
      "        -2.4807e-02, -1.9157e-02, -1.9972e-02,  5.3540e-03, -4.0255e-02,\n",
      "        -3.5989e-02,  3.6633e-02, -2.4697e-02,  9.5303e-03,  4.7737e-02,\n",
      "         1.9159e-02, -3.1676e-02,  2.9102e-03, -3.2972e-02,  3.4784e-02,\n",
      "        -1.1361e-02, -7.4205e-03, -4.8485e-02, -4.5568e-02,  2.3733e-02,\n",
      "        -1.4198e-02,  2.0609e-02,  4.2485e-02,  1.6501e-02,  3.2936e-02,\n",
      "        -4.1279e-03, -1.9662e-02, -3.3918e-02,  3.6629e-02,  3.7996e-02,\n",
      "         3.9135e-02,  4.2955e-03, -2.7090e-02,  1.7885e-02,  1.0778e-02,\n",
      "        -3.3524e-02,  2.2656e-02,  3.2203e-02,  2.5892e-02,  4.3104e-02,\n",
      "        -3.6449e-02,  4.7898e-02,  3.0714e-03, -4.5865e-04, -2.1793e-02,\n",
      "        -3.4271e-02,  2.1345e-02, -4.5779e-02,  3.3857e-02,  1.3767e-02])), ('neural_net.2.weight', tensor([[-9.8502e-04, -3.5102e-02, -2.9225e-03,  ...,  3.4163e-02,\n",
      "          2.6479e-02, -3.0171e-02],\n",
      "        [-3.2332e-02,  3.9633e-03, -4.0618e-03,  ...,  3.2624e-02,\n",
      "         -2.3111e-02,  1.2824e-02],\n",
      "        [ 6.8844e-03,  1.4888e-02,  2.4755e-03,  ...,  2.7248e-02,\n",
      "         -2.8355e-02, -3.7723e-03],\n",
      "        ...,\n",
      "        [-2.4318e-02,  1.2271e-02,  9.4771e-03,  ..., -2.4490e-02,\n",
      "         -3.8543e-03,  2.2274e-02],\n",
      "        [ 1.5967e-03, -1.2049e-02, -2.0770e-02,  ..., -2.8042e-02,\n",
      "         -4.8260e-03, -6.7802e-05],\n",
      "        [ 1.6636e-02, -1.2233e-02,  2.1287e-03,  ...,  3.0547e-02,\n",
      "          6.0805e-04, -1.2025e-02]])), ('neural_net.2.bias', tensor([ 0.0340, -0.0164, -0.0109, -0.0154, -0.0114,  0.0061,  0.0321,  0.0284,\n",
      "        -0.0007,  0.0210,  0.0284,  0.0253, -0.0276,  0.0105, -0.0224, -0.0040,\n",
      "         0.0342, -0.0102,  0.0128, -0.0308, -0.0311,  0.0020,  0.0294, -0.0217,\n",
      "        -0.0035, -0.0138, -0.0321,  0.0226, -0.0299, -0.0152,  0.0280,  0.0152,\n",
      "         0.0017, -0.0264, -0.0015, -0.0152, -0.0314,  0.0118,  0.0156,  0.0284,\n",
      "         0.0141, -0.0203, -0.0156, -0.0137, -0.0104, -0.0126, -0.0337, -0.0338,\n",
      "         0.0189, -0.0034, -0.0065,  0.0008, -0.0257, -0.0210,  0.0203, -0.0177,\n",
      "         0.0028,  0.0321, -0.0235,  0.0207,  0.0076,  0.0180,  0.0111, -0.0019,\n",
      "        -0.0285, -0.0127, -0.0284, -0.0198, -0.0013, -0.0054,  0.0288,  0.0258,\n",
      "        -0.0238,  0.0152,  0.0203, -0.0133,  0.0093,  0.0049,  0.0195,  0.0249,\n",
      "         0.0245, -0.0169,  0.0315, -0.0135, -0.0143,  0.0016,  0.0081, -0.0109,\n",
      "         0.0078,  0.0061, -0.0288,  0.0210, -0.0114, -0.0161,  0.0114,  0.0216,\n",
      "        -0.0012, -0.0198,  0.0338,  0.0016,  0.0171, -0.0005, -0.0243,  0.0283,\n",
      "        -0.0054, -0.0349, -0.0137, -0.0038, -0.0086,  0.0045, -0.0015, -0.0164,\n",
      "        -0.0269,  0.0038,  0.0187, -0.0244,  0.0226,  0.0215, -0.0040,  0.0188,\n",
      "         0.0331,  0.0348, -0.0172, -0.0051,  0.0207, -0.0106,  0.0261,  0.0270])), ('neural_net.4.weight', tensor([[ 0.0121,  0.0658,  0.0854,  ...,  0.0177, -0.0768,  0.0450],\n",
      "        [-0.0319, -0.0648, -0.0068,  ...,  0.0232,  0.0442,  0.0240],\n",
      "        [-0.0570, -0.0699,  0.0536,  ..., -0.0060, -0.0527, -0.0490],\n",
      "        ...,\n",
      "        [-0.0134, -0.0616, -0.0182,  ..., -0.0043, -0.0120,  0.0609],\n",
      "        [-0.0316, -0.0045,  0.0442,  ..., -0.0551,  0.0214,  0.0525],\n",
      "        [ 0.0557,  0.0285, -0.0808,  ..., -0.0022,  0.0768, -0.0119]])), ('neural_net.4.bias', tensor([-0.0602, -0.0352,  0.0113,  0.0845,  0.0618, -0.0816, -0.0865, -0.0820,\n",
      "         0.0493, -0.0378, -0.0150, -0.0491, -0.0244,  0.0260,  0.0409, -0.0199,\n",
      "        -0.0247,  0.0756, -0.0383, -0.0437,  0.0416,  0.0379, -0.0436, -0.0395,\n",
      "         0.0831, -0.0088,  0.0328,  0.0432,  0.0787, -0.0680, -0.0837, -0.0831,\n",
      "         0.0674, -0.0259,  0.0733]))])\n",
      "=================================\n",
      "label is LWTN\n",
      "label is SLBP\n",
      "label is VXXA\n",
      "label is YEGG\n",
      "label is 8U6B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/p23shi/.local/lib/python3.7/site-packages/torch/nn/modules/container.py:117: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n",
      "/home/p23shi/.local/lib/python3.7/site-packages/torch/nn/modules/container.py:117: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n",
      "/home/p23shi/.local/lib/python3.7/site-packages/torch/autograd/__init__.py:132: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  allow_unreachable=True)  # allow_unreachable flag\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label is DUUX\n",
      "label is E7HY\n",
      "label is HZD5\n",
      "label is DVJC\n",
      "label is MQQC\n",
      "label is 97GN\n",
      "label is BCQQ\n",
      "label is 6Z2B\n",
      "label is NHLW\n",
      "label is 2XF7\n",
      "label is C4HR\n",
      "label is BL3Y\n",
      "label is V3TZ\n",
      "label is YE34\n",
      "label is QK2V\n",
      "label is YF79\n",
      "label is KY8P\n",
      "label is LGB9\n",
      "label is ED7B\n",
      "label is W4F9\n",
      "label is 3J38\n",
      "label is B7E5\n",
      "label is GGSC\n",
      "label is NV7Z\n",
      "label is PWLF\n",
      "label is 6L5Y\n",
      "label is 9VAE\n",
      "label is BJXY\n",
      "label is UZR9\n",
      "label is QJD7\n",
      "label is 6P5Y\n",
      "label is WKYM\n",
      "label is 5D9J\n",
      "label is GYVE\n",
      "label is 7CNG\n",
      "label is 8VFD\n",
      "label is CSAX\n",
      "label is 373H\n",
      "label is RNW5\n",
      "label is CKJU\n",
      "label is NW3S\n",
      "label is L46V\n",
      "label is BBT5\n",
      "label is VSH5\n",
      "label is W96G\n",
      "label is M6LP\n",
      "label is NNMM\n",
      "label is RENL\n",
      "label is 6Q27\n",
      "label is 3GYK\n",
      "label is 7W48\n",
      "label is U55R\n",
      "label is 2TNL\n",
      "label is CU56\n",
      "label is 29DD\n",
      "label is YXW2\n",
      "label is BCUW\n",
      "label is FALW\n",
      "label is E4N7\n",
      "at 1000 step loss is 104959288102031.56\n",
      "in 250 tests, correct chars is 175\n",
      "in 250 tests, correct predicted number of chars 241\n",
      "in 250 tests, correct predicted words 0\n",
      "at 2000 step loss is 59762026824625.12\n",
      "in 250 tests, correct chars is 380\n",
      "in 250 tests, correct predicted number of chars 244\n",
      "in 250 tests, correct predicted words 9\n",
      "at 3000 step loss is 41575882464158.19\n",
      "in 250 tests, correct chars is 443\n",
      "in 250 tests, correct predicted number of chars 240\n",
      "in 250 tests, correct predicted words 14\n",
      "at 4000 step loss is 32273003642260.977\n",
      "in 250 tests, correct chars is 452\n",
      "in 250 tests, correct predicted number of chars 245\n",
      "in 250 tests, correct predicted words 8\n",
      "at 5000 step loss is 26600804473111.54\n",
      "in 250 tests, correct chars is 493\n",
      "in 250 tests, correct predicted number of chars 245\n",
      "in 250 tests, correct predicted words 17\n",
      "at 6000 step loss is 22715589661397.223\n",
      "in 250 tests, correct chars is 584\n",
      "in 250 tests, correct predicted number of chars 249\n",
      "in 250 tests, correct predicted words 31\n",
      "at 7000 step loss is 19899429528240.918\n",
      "in 250 tests, correct chars is 623\n",
      "in 250 tests, correct predicted number of chars 244\n",
      "in 250 tests, correct predicted words 38\n",
      "at 8000 step loss is 17745307475607.293\n",
      "in 250 tests, correct chars is 686\n",
      "in 250 tests, correct predicted number of chars 245\n",
      "in 250 tests, correct predicted words 52\n",
      "at 9000 step loss is 16017962546833.123\n",
      "in 250 tests, correct chars is 790\n",
      "in 250 tests, correct predicted number of chars 244\n",
      "in 250 tests, correct predicted words 104\n",
      "at 10000 step loss is 14530542854180.521\n",
      "in 250 tests, correct chars is 799\n",
      "in 250 tests, correct predicted number of chars 237\n",
      "in 250 tests, correct predicted words 114\n",
      "at 11000 step loss is 13305574469064.35\n",
      "in 250 tests, correct chars is 786\n",
      "in 250 tests, correct predicted number of chars 242\n",
      "in 250 tests, correct predicted words 105\n",
      "at 12000 step loss is 12280028549254.877\n",
      "in 250 tests, correct chars is 826\n",
      "in 250 tests, correct predicted number of chars 239\n",
      "in 250 tests, correct predicted words 134\n"
     ]
    }
   ],
   "source": [
    "\n",
    "vocabulary = {\n",
    "    \"1\" : 1,\n",
    "    \"2\" : 2,\n",
    "    \"3\" : 3,\n",
    "    \"4\" : 4,\n",
    "    \"5\" : 5,\n",
    "    \"6\" : 6,\n",
    "    \"7\" : 7,\n",
    "    \"8\" : 8,\n",
    "    \"9\" : 9,\n",
    "    \"a\" : 10,\n",
    "    \"b\" : 11,\n",
    "    \"c\" : 12,\n",
    "    \"d\" : 13,\n",
    "    \"e\" : 14,\n",
    "    \"f\" : 15,\n",
    "    \"g\" : 16,\n",
    "    \"h\" : 17,\n",
    "    \"i\" : 18,\n",
    "    \"j\" : 19,\n",
    "    \"k\" : 20,\n",
    "    \"l\" : 21,\n",
    "    \"m\" : 22,\n",
    "    \"n\" : 23,\n",
    "    \"o\" : 24,\n",
    "    \"p\" : 25,\n",
    "    \"q\" : 26,\n",
    "    \"r\" : 27,\n",
    "    \"s\" : 28,\n",
    "    \"t\" : 29,\n",
    "    \"u\" : 30,\n",
    "    \"v\" : 31,\n",
    "    \"w\" : 32, \n",
    "    \"x\" : 33,\n",
    "    \"y\" : 34,\n",
    "    \"z\" : 35\n",
    "}\n",
    "vocabulary_size = len(vocabulary)\n",
    "\n",
    "MAXCHAR = 10 # assume a captcha has at most 10 chars\n",
    "\n",
    "# p(N | img)\n",
    "class NumNet(nn.Module):\n",
    "    def __init__(self, img_size, out_size = 10):\n",
    "        super(NumNet, self).__init__()\n",
    "        self.neural_net = nn.Sequential(\n",
    "            nn.Linear(img_size[0] * img_size[1], img_size[0] * img_size[1] * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(img_size[0] * img_size[1] * 2, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, out_size),\n",
    "            nn.LogSoftmax())\n",
    "        \n",
    "    def forward(self, img):\n",
    "        img = torch.reshape(img, (-1,))\n",
    "        prob = self.neural_net(img)\n",
    "        return prob\n",
    "\n",
    "# p(c | img)\n",
    "class CharNet(nn.Module):\n",
    "    def __init__(self, img_size, out_size=vocabulary_size, noise=None):\n",
    "        super(CharNet, self).__init__()\n",
    "        self.neural_net = nn.Sequential(\n",
    "            nn.Linear(img_size[0] * img_size[1], img_size[0] * img_size[1] * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(img_size[0] * img_size[1] * 2, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, out_size),\n",
    "            nn.Softmax())\n",
    "\n",
    "    def forward(self, img, noise=None):\n",
    "        img = torch.reshape(img, (-1,))\n",
    "        prob = self.neural_net(img)\n",
    "        return prob\n",
    "\n",
    "class CaptchaSolver(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.numNet = NumNet((24, 72), MAXCHAR)\n",
    "        self.charNet = CharNet((20, 20), vocabulary_size)\n",
    "        self.epi = 0\n",
    "        \n",
    "    def model(self, original_img, imgs, n_char, charList):\n",
    "        pyro.module(\"captchasolver\", self)\n",
    "        num_p = torch.tensor(1 / MAXCHAR).repeat(MAXCHAR)\n",
    "        N = pyro.sample(\"num_char\", dist.Categorical(num_p)).float()\n",
    "        pyro.sample(\"num_char_obs\", dist.Normal(N, torch.tensor(1e-6)), obs=n_char)\n",
    "        \n",
    "        sampled_c_probs = []\n",
    "        for i in range(1, int(n_char) + 1):\n",
    "            c_prob = torch.tensor(1 / vocabulary_size).repeat(vocabulary_size)\n",
    "            c_prob = torch.unsqueeze(c_prob, dim=0)\n",
    "            sampled_c_probs.append(c_prob)\n",
    "        c_probs = torch.cat(sampled_c_probs, dim=0)\n",
    "        chars = pyro.sample(\"chars\", dist.Categorical(c_probs)).float()\n",
    "        sigmas = torch.tensor([1e-6, 1e-6, 1e-6, 1e-6])\n",
    "        pyro.sample(\"chars_obs\", dist.Normal(chars, sigmas), obs=charList)\n",
    "    \n",
    "    def guide(self, original_img, imgs, n_char, charList):\n",
    "        pyro.module(\"captchasolver\", self)\n",
    "        num_p = self.numNet(original_img)\n",
    "        N = pyro.sample(\"num_char\", dist.Categorical(num_p)).float()\n",
    "        sampled_c_probs = []\n",
    "        for i in range(int(n_char)):\n",
    "            segmented_img = imgs[i]\n",
    "            c_prob = self.charNet(segmented_img)\n",
    "            \n",
    "            c_prob = torch.unsqueeze(c_prob, dim=0)\n",
    "            sampled_c_probs.append(c_prob)\n",
    "        c_probs = torch.cat(sampled_c_probs, dim=0)\n",
    "        chars = pyro.sample(\"chars\", dist.Categorical(c_probs)).float()\n",
    "        sigmas = torch.tensor([0.05, 0.05, 0.05, 0.05]).float()\n",
    "\n",
    "captchaSolver = CaptchaSolver()\n",
    "model = captchaSolver.model\n",
    "guide = captchaSolver.guide\n",
    "learning_rate = 2e-4\n",
    "optimizer = optim.Adam({\"lr\":learning_rate})\n",
    "svi = SVI(model, guide, optimizer, loss=Trace_ELBO())\n",
    "\n",
    "def optimize():\n",
    "    loss = 0\n",
    "    num_steps = 80000\n",
    "    print(\"Optimizing...\")\n",
    "    for t in range(num_steps):\n",
    "        loss += inference(t)\n",
    "        if (t % 1000 == 0) and (t > 0):\n",
    "            print(\"at {} step loss is {}\".format(t, loss / t))\n",
    "            run_test(n_test=250, mute=True)\n",
    "\n",
    "def inference(t):\n",
    "    global DATA, RAW_DATA\n",
    "    batchNum = 64\n",
    "    loss = 0\n",
    "    batchFileNames = random.sample(list(DATA), batchNum)\n",
    "    batchImages = [ DATA[k] for k in batchFileNames ]\n",
    "    batchOriImages = [ RAW_DATA[k] for k in batchFileNames ]\n",
    "    for i in range(batchNum):\n",
    "        label = batchFileNames[i]\n",
    "        if t == 0:\n",
    "            print(\"label is\", label)\n",
    "        label = convert_label_to_indexList(label)\n",
    "        loss += svi.step(batchOriImages[i], batchImages[i], torch.tensor(len(label)).float(), label) / batchNum\n",
    "    return loss\n",
    "\n",
    "def convert_label_to_indexList(label):\n",
    "    indexList = []\n",
    "    for char in label:\n",
    "        indexList.append(vocabulary[char.lower()] - 1)\n",
    "    return torch.tensor(indexList).float()\n",
    "\n",
    "# compute the global mean of the images\n",
    "def calculate_global_mean():\n",
    "    imgs = []\n",
    "    for file in filenames:\n",
    "        sample_img_gray = cv.imread(os.path.join(sampleFolder, file), cv.IMREAD_GRAYSCALE)\n",
    "        sample_img_gray = np.expand_dims(sample_img_gray, axis=0)\n",
    "        imgs.append(sample_img_gray)\n",
    "    imgs = np.concatenate(imgs)\n",
    "    return np.mean(imgs)\n",
    "\n",
    "GLOBAL_MEAN = calculate_global_mean()\n",
    "print(\"GLOBAL_MEAN is\", GLOBAL_MEAN)\n",
    "\n",
    "def preprocess_image(img, gd_truth_label=\"\", TRAIN=True, saveFolder=None):\n",
    "    letter_images, letter_labels = split_image(img, gd_truth_label, TRAIN)\n",
    "    if letter_images is None or letter_labels is None:\n",
    "        return None, None\n",
    "    resized_letter_images = []\n",
    "    for letter_image, letter_label in zip(letter_images, letter_labels):\n",
    "        letter_image = letter_image.astype(np.uint8) # convert type to uint8\n",
    "        letter_image = resize_to_fit(letter_image, width=20, height=20)\n",
    "        if saveFolder: # save the extracted and resized image\n",
    "            # Get the folder to save the image in\n",
    "            save_dir = os.path.join(saveFolder, letter_label)\n",
    "            \n",
    "            # if the output directory does not exist, create it\n",
    "            if not os.path.exists(save_dir):\n",
    "                os.makedirs(save_dir)\n",
    "\n",
    "            # write the letter image to a file\n",
    "            save_path = os.path.join(save_dir, \"{}.png\".format(uuid.uuid4()))\n",
    "            cv2.imwrite(save_path, letter_image)\n",
    "        \n",
    "        letter_image = torch.from_numpy(letter_image - GLOBAL_MEAN).float() / 255 # normalize\n",
    "        resized_letter_images.append(letter_image)\n",
    "            \n",
    "    return resized_letter_images, letter_labels\n",
    "\n",
    "import time\n",
    "\n",
    "DATA = {} # scaled/cropped input\n",
    "RAW_DATA = {} # original input \n",
    "def prepare_data():\n",
    "    global DATA, RAW_DATA\n",
    "    for file in filenames:\n",
    "        sample_img_gray = cv.imread(os.path.join(sampleFolder, file), cv.IMREAD_GRAYSCALE)\n",
    "        gd_truth_label = file.split(\".\")[0]\n",
    "        resized_letter_images, letter_labels = preprocess_image(sample_img_gray, gd_truth_label, TRAIN=True, saveFolder=None)\n",
    "        if resized_letter_images is None or letter_labels is None:\n",
    "            continue\n",
    "        sample_img_gray = torch.from_numpy(sample_img_gray - GLOBAL_MEAN).float() / 255\n",
    "        DATA[gd_truth_label] = resized_letter_images\n",
    "        RAW_DATA[gd_truth_label] = sample_img_gray\n",
    "\n",
    "def check_data():\n",
    "    global DATA\n",
    "    for k, v in DATA.items():\n",
    "        assert len(k) == 4\n",
    "        assert len(v) == 4\n",
    "        for sub_img in v:\n",
    "            assert int(sub_img.shape[0]) == 20\n",
    "            assert int(sub_img.shape[1]) == 20\n",
    "\n",
    "\n",
    "def predict_char(img):\n",
    "    n_sample = 20\n",
    "    outs = []\n",
    "    for i in range(n_sample):\n",
    "        c_prob = captchaSolver.charNet(img)\n",
    "        char = dist.Categorical(c_prob).sample()\n",
    "        outs.append(char)\n",
    "    return round(np.mean(np.array(outs)))\n",
    "\n",
    "def predict_num(img):\n",
    "    n_sample = 20\n",
    "    outs = []\n",
    "    for i in range(n_sample):\n",
    "        n_prob = captchaSolver.numNet(img)\n",
    "        N = int(dist.Categorical(n_prob).sample())\n",
    "        outs.append(N)\n",
    "    return round(np.mean(np.array(outs)))            \n",
    "            \n",
    "def test(testFileName, mute=False):\n",
    "    test_img_gray = cv.imread(os.path.join(sampleFolder, testFileName), cv.IMREAD_GRAYSCALE)\n",
    "    groundtruth = testFileName.split('.')[0]\n",
    "    \n",
    "    if not mute:\n",
    "        print(\"testing expected label is\", groundtruth)\n",
    "    trueCharList =  convert_label_to_indexList(groundtruth)\n",
    "    if not mute:\n",
    "        print(\"testing expected indices are\", trueCharList)\n",
    "    resized_letter_images, letter_labels = preprocess_image(test_img_gray, groundtruth)\n",
    "    if resized_letter_images is None or letter_labels is None:\n",
    "        return 0, 0, 0\n",
    "    \n",
    "    test_img_gray = torch.from_numpy(test_img_gray - GLOBAL_MEAN).float() / 255\n",
    "    N = predict_num(test_img_gray)\n",
    "    if not mute:\n",
    "        print(\"testing N is\", N)\n",
    "    \n",
    "    corre_num = 1 if N == len(trueCharList) else 0\n",
    "\n",
    "    chars = []\n",
    "    for i in range(min(N, len(resized_letter_images))):\n",
    "        segmented_img = resized_letter_images[i]\n",
    "        char = predict_char(segmented_img)\n",
    "        chars.append(char)\n",
    "        \n",
    "    if not mute:\n",
    "        print(\"testing actual indices are\", chars)\n",
    "    count = 0\n",
    "    \n",
    "    for i in range(min(len(chars), len(trueCharList))):\n",
    "        if int(chars[i]) == int(trueCharList[i]):\n",
    "            count += 1\n",
    "    \n",
    "    correct_word = 1 if count == len(trueCharList) else 0\n",
    "    return count, corre_num, correct_word\n",
    "\n",
    "def run_test(n_test=250, mute=False):\n",
    "    \n",
    "    NUM_TEST = n_test\n",
    "    correct_num = 0\n",
    "    correct_chars = 0\n",
    "    correct_words = 0\n",
    "    for i in range(NUM_TEST):\n",
    "        testFileName = random.sample(test_filenames, 1)[0]\n",
    "        correct_c, corrent_n, correct_word = test(testFileName, mute)\n",
    "        correct_chars += correct_c\n",
    "        correct_num += corrent_n\n",
    "        correct_words += correct_word\n",
    "\n",
    "    print(\"in %d tests, correct chars is\" % NUM_TEST, correct_chars)\n",
    "    print(\"in %d tests, correct predicted number of chars\" % NUM_TEST, correct_num)\n",
    "    print(\"in %d tests, correct predicted words\" % NUM_TEST, correct_words)\n",
    "       \n",
    "prepare_data()\n",
    "check_data()\n",
    "print(\"KVP in data processed:\", len(DATA))\n",
    "\n",
    "optimize()\n",
    "\n",
    "run_test()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
