{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "banner-smith",
   "metadata": {
    "id": "banner-smith"
   },
   "source": [
    "uncomment if executing in google colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "center-syria",
   "metadata": {
    "id": "center-syria"
   },
   "outputs": [],
   "source": [
    "! pip uninstall pyro-ppl\n",
    "! pip install pyro-ppl==1.5.1\n",
    "! unzip fonts.zip \n",
    "! unzip claptchagen.zip\n",
    "! unzip csis\n",
    "! cp csis.py /usr/local/lib/python3.7/dist-packages/pyro/infer\n",
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ktlsxvi5VS-Q",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ktlsxvi5VS-Q",
    "outputId": "b111eefc-1b3e-44b1-ae1c-3211c886036e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue May 11 21:33:43 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 465.19.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   57C    P0    29W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n",
      "processor\t: 0\n",
      "vendor_id\t: GenuineIntel\n",
      "cpu family\t: 6\n",
      "model\t\t: 79\n",
      "model name\t: Intel(R) Xeon(R) CPU @ 2.20GHz\n",
      "stepping\t: 0\n",
      "microcode\t: 0x1\n",
      "cpu MHz\t\t: 2199.998\n",
      "cache size\t: 56320 KB\n",
      "physical id\t: 0\n",
      "siblings\t: 2\n",
      "core id\t\t: 0\n",
      "cpu cores\t: 1\n",
      "apicid\t\t: 0\n",
      "initial apicid\t: 0\n",
      "fpu\t\t: yes\n",
      "fpu_exception\t: yes\n",
      "cpuid level\t: 13\n",
      "wp\t\t: yes\n",
      "flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities\n",
      "bugs\t\t: cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs taa\n",
      "bogomips\t: 4399.99\n",
      "clflush size\t: 64\n",
      "cache_alignment\t: 64\n",
      "address sizes\t: 46 bits physical, 48 bits virtual\n",
      "power management:\n",
      "\n",
      "processor\t: 1\n",
      "vendor_id\t: GenuineIntel\n",
      "cpu family\t: 6\n",
      "model\t\t: 79\n",
      "model name\t: Intel(R) Xeon(R) CPU @ 2.20GHz\n",
      "stepping\t: 0\n",
      "microcode\t: 0x1\n",
      "cpu MHz\t\t: 2199.998\n",
      "cache size\t: 56320 KB\n",
      "physical id\t: 0\n",
      "siblings\t: 2\n",
      "core id\t\t: 0\n",
      "cpu cores\t: 1\n",
      "apicid\t\t: 1\n",
      "initial apicid\t: 1\n",
      "fpu\t\t: yes\n",
      "fpu_exception\t: yes\n",
      "cpuid level\t: 13\n",
      "wp\t\t: yes\n",
      "flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities\n",
      "bugs\t\t: cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs taa\n",
      "bogomips\t: 4399.99\n",
      "clflush size\t: 64\n",
      "cache_alignment\t: 64\n",
      "address sizes\t: 46 bits physical, 48 bits virtual\n",
      "power management:\n",
      "\n",
      "MemTotal:       13305332 kB\n",
      "MemFree:         9337936 kB\n",
      "MemAvailable:   12441280 kB\n",
      "Buffers:           90072 kB\n",
      "Cached:          3138468 kB\n",
      "SwapCached:            0 kB\n",
      "Active:          1027436 kB\n",
      "Inactive:        2642600 kB\n",
      "Active(anon):     399048 kB\n",
      "Inactive(anon):      432 kB\n",
      "Active(file):     628388 kB\n",
      "Inactive(file):  2642168 kB\n",
      "Unevictable:           0 kB\n",
      "Mlocked:               0 kB\n",
      "SwapTotal:             0 kB\n",
      "SwapFree:              0 kB\n",
      "Dirty:               744 kB\n",
      "Writeback:             0 kB\n",
      "AnonPages:        441384 kB\n",
      "Mapped:           228012 kB\n",
      "Shmem:              1192 kB\n",
      "KReclaimable:     148520 kB\n",
      "Slab:             200392 kB\n",
      "SReclaimable:     148520 kB\n",
      "SUnreclaim:        51872 kB\n",
      "KernelStack:        4976 kB\n",
      "PageTables:         5876 kB\n",
      "NFS_Unstable:          0 kB\n",
      "Bounce:                0 kB\n",
      "WritebackTmp:          0 kB\n",
      "CommitLimit:     6652664 kB\n",
      "Committed_AS:    3294252 kB\n",
      "VmallocTotal:   34359738367 kB\n",
      "VmallocUsed:       47196 kB\n",
      "VmallocChunk:          0 kB\n",
      "Percpu:             1424 kB\n",
      "AnonHugePages:         0 kB\n",
      "ShmemHugePages:        0 kB\n",
      "ShmemPmdMapped:        0 kB\n",
      "FileHugePages:         0 kB\n",
      "FilePmdMapped:         0 kB\n",
      "HugePages_Total:       0\n",
      "HugePages_Free:        0\n",
      "HugePages_Rsvd:        0\n",
      "HugePages_Surp:        0\n",
      "Hugepagesize:       2048 kB\n",
      "Hugetlb:               0 kB\n",
      "DirectMap4k:      208072 kB\n",
      "DirectMap2M:     6082560 kB\n",
      "DirectMap1G:     9437184 kB\n"
     ]
    }
   ],
   "source": [
    "! nvidia-smi\n",
    "! cat /proc/cpuinfo\n",
    "! cat /proc/meminfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intimate-postage",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "intimate-postage",
    "outputId": "dbf90bd3-bf92-464b-e862-c7e682ccc9f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5.1\n",
      "number of samples in group 0\n",
      "number of samples in group 0\n",
      "number of samples in group 0\n",
      "number of samples in group 4979\n",
      "text 1466 captcha shape (32, 100) noise 0.027794714300541375\n",
      "text 8671 captcha shape (32, 100) noise 0.0959986494600341\n",
      "text 4963 captcha shape (32, 100) noise 0.6533736664141644\n",
      "text 2108 captcha shape (32, 100) noise 0.6441049233359232\n",
      "text 1336 captcha shape (32, 100) noise 0.6234566413620279\n",
      "text 8347 captcha shape (32, 100) noise 0.6571003544576776\n",
      "text 3870 captcha shape (32, 100) noise 0.5857075384878575\n",
      "text 0648 captcha shape (32, 100) noise 0.652269429332352\n",
      "text 9138 captcha shape (32, 100) noise 0.5489655717240042\n",
      "text 2032 captcha shape (32, 100) noise 0.930978747781022\n",
      "text 3374 captcha shape (32, 100) noise 0.36559113198422505\n",
      "number of samples in group 5021\n",
      "text 23498 captcha shape (32, 100) noise 0.26544658068345206\n",
      "text 42946 captcha shape (32, 100) noise 0.70699093631268\n",
      "text 53675 captcha shape (32, 100) noise 0.6200182076176314\n",
      "text 91709 captcha shape (32, 100) noise 0.69708688394102\n",
      "text 33153 captcha shape (32, 100) noise 0.6399715032378838\n",
      "text 25256 captcha shape (32, 100) noise 0.039647585595412795\n",
      "text 21838 captcha shape (32, 100) noise 0.8389278210039006\n",
      "text 59652 captcha shape (32, 100) noise 0.5510196180365019\n",
      "text 11565 captcha shape (32, 100) noise 0.7203038798278182\n",
      "text 92289 captcha shape (32, 100) noise 0.14273932225474828\n",
      "text 76337 captcha shape (32, 100) noise 0.2600331272999933\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import string\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pyro\n",
    "import numpy as np\n",
    "import pyro.optim as optim\n",
    "import pyro.distributions as dist\n",
    "import pyro.infer\n",
    "import pyro.optim\n",
    "import time\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from pyro.infer import SVI, Trace_ELBO, TraceGraph_ELBO\n",
    "from PIL import Image\n",
    "from claptchagen.claptcha import Claptcha\n",
    "from torch.distributions import constraints\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import matplotlib.pyplot as plt\n",
    "print(pyro.__version__)\n",
    "\n",
    "captcha_folder = 'generated_captchas'\n",
    "captchaHeight = 32\n",
    "captchaWidth = 100\n",
    "captchaMarginX = 4\n",
    "captchaMarginY = 4\n",
    "batch_size = 10\n",
    "\n",
    "char_dict = string.digits\n",
    "USE_CUDA = True\n",
    "MAX_N = 5 # maximum number of letters in a captcha \n",
    "MIN_N = 4 # minimum number of letters in a captcha\n",
    "MIN_NOISE = 0.01 # minimum noise\n",
    "MAX_NOISE = 0.99 # maximum noise\n",
    "smoke_test = False\n",
    "num_steps = 200 if not smoke_test else 10\n",
    "TrainingSample = 10000 if not smoke_test else 100 # number of captchas generated for training\n",
    "\n",
    "def randomString():\n",
    "    \"\"\"\n",
    "    return a string with <num_char> random letters\n",
    "    \"\"\"\n",
    "    k = random.randint(MIN_N, MAX_N) # sample number of characters\n",
    "    \n",
    "    rndLetters = (random.choice(char_dict) for _ in range(k))\n",
    "    \n",
    "    pad_spaces = MAX_N - k # pad the string so the captcha is close to center\n",
    "    space = \" \" * (pad_spaces // 2)\n",
    "    \n",
    "    return space + \"\".join(rndLetters) + space\n",
    "\n",
    "def ramdomNoise():\n",
    "    \"\"\"\n",
    "    return a float between MIN_NOISE, MAX_NOISE\n",
    "    \"\"\"\n",
    "    return random.uniform(MIN_NOISE, MAX_NOISE)\n",
    "\n",
    "def generate_random_captcha(n, save=False):\n",
    "    \"\"\"\n",
    "    generate n random captchas,\n",
    "    return a list of texts on the captchas\n",
    "    \"\"\"\n",
    "    # Initialize Claptcha object with random text, FreeMono as font, of size\n",
    "    # 100x30px, using bicubic resampling filter and adding a bit of white noise\n",
    "    c = Claptcha(randomString, \"fonts/FreeSans.ttf\", (captchaWidth, captchaHeight), (captchaMarginX, captchaMarginY),\n",
    "             resample=Image.BILINEAR, noise=0)\n",
    "    captcha_generated = [ [] for i in range(MAX_N)]\n",
    "    for i in range(n):\n",
    "        c.noise = ramdomNoise()\n",
    "        if save:\n",
    "            text, _ = c.write(os.path.join(captcha_folder, 'captcha{}.png'.format(i)))\n",
    "            os.rename(os.path.join(captcha_folder, 'captcha{}.png'.format(i)),os.path.join(captcha_folder, '{}.png'.format(text + \"_\" + str(i))))\n",
    "        text, image = c.image\n",
    "        text = text.strip()\n",
    "        image = np.array(image)[:, :, 0] # the generator is gray scale, only keep one channel is enough\n",
    "        captcha_generated[len(text) - 1].append((text, image, c.noise))\n",
    "    return captcha_generated\n",
    "    \n",
    "captcha_generated = generate_random_captcha(TrainingSample, save=False)\n",
    "for lst in captcha_generated:\n",
    "    print(\"number of samples in group\", len(lst))\n",
    "    # print some sample captcha information generated\n",
    "    for i, t in enumerate(lst):\n",
    "        print(\"text\", t[0], \"captcha shape\", t[1].shape, \"noise\", t[2])\n",
    "        if i >= 10:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "negative-underwear",
   "metadata": {
    "id": "negative-underwear"
   },
   "outputs": [],
   "source": [
    "def render_image(chars, fonts=\"fonts/FreeSans.ttf\", size=(captchaWidth, captchaHeight), \n",
    "                 margin=(captchaMarginX, captchaMarginY), resample=Image.BILINEAR, noise=0.3, use_cuda=False):\n",
    "    #noise = noise.data.item()\n",
    "    #print(chars, noise)\n",
    "    pad_spaces = MAX_N - len(chars)\n",
    "    space = \" \" * (pad_spaces // 2)\n",
    "    chars = space + chars + space\n",
    "    render = Claptcha(chars, fonts, size, margin, resample=resample, noise=noise)\n",
    "\n",
    "    \n",
    "    _ , rendered_image = render.image\n",
    "    rendered_image = np.array(rendered_image)[:,:,0] # the generator is gray scale, only keep one channel is enough\n",
    "    rendered_image = np.subtract(np.divide(rendered_image, 255), 0.5)\n",
    "    rendered_image = torch.from_numpy(rendered_image)\n",
    "    if use_cuda:\n",
    "        rendered_image = rendered_image.cuda()\n",
    "    return rendered_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "contrary-lobby",
   "metadata": {
    "id": "contrary-lobby"
   },
   "outputs": [],
   "source": [
    "class CaptchaDataset(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, raw_captchas, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.raw_captchas = raw_captchas\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.raw_captchas)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        label = self.raw_captchas[idx][0]\n",
    "        image = self.raw_captchas[idx][1]\n",
    "        noise = self.raw_captchas[idx][2]\n",
    "        \n",
    "        image = np.subtract(np.divide(image, 255), 0.5)\n",
    "        image = torch.from_numpy(image).float()\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return label, image, noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "taken-spice",
   "metadata": {
    "id": "taken-spice"
   },
   "outputs": [],
   "source": [
    "def make_loarders(BATCH_SIZE, raw_samples):\n",
    "    dataloaders = [] # dataloaders for different num of char\n",
    "    for lst in raw_samples:\n",
    "        if lst:\n",
    "            ds = CaptchaDataset(lst)\n",
    "            dataloader = DataLoader(ds, batch_size=BATCH_SIZE,\n",
    "                                    shuffle=True, num_workers=0, drop_last=True)\n",
    "            dataloaders.append(dataloader)\n",
    "    return dataloaders\n",
    "\n",
    "def make_batches(dataloaders):\n",
    "    all_batches = []\n",
    "    for dl in dataloaders:\n",
    "        for i_batch, sample in enumerate(dl):\n",
    "            all_batches.append(sample)\n",
    "    random.shuffle(all_batches)\n",
    "    random.shuffle(all_batches)\n",
    "    return all_batches\n",
    "\n",
    "TrainLoaders = make_loarders(BATCH_SIZE=batch_size, raw_samples=captcha_generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arabic-rocket",
   "metadata": {
    "id": "arabic-rocket"
   },
   "outputs": [],
   "source": [
    "class NoiseNet(nn.Module):\n",
    "\n",
    "    def __init__(self, img_size, out_size = 1):\n",
    "        \"\"\"\n",
    "        Network for learning noise in a captcha\n",
    "        \"\"\"\n",
    "        super(NoiseNet, self).__init__()\n",
    "        \n",
    "        self.img_size = img_size\n",
    "        self.fc0 = nn.Linear(img_size[0] * img_size[1], img_size[0] * img_size[1])\n",
    "        self.fc1 = nn.Linear(img_size[0] * img_size[1], 1024)\n",
    "        \n",
    "        self.fc2 = nn.Linear(1024, 1024)\n",
    "        self.fc20 = nn.Linear(1024, img_size[0] * img_size[1])\n",
    "        self.fc21 = nn.Linear(img_size[0] * img_size[1], out_size)\n",
    "        self.softplus = nn.Softplus()\n",
    "    \n",
    "    def forward(self, img):\n",
    "        BS = img.shape[0]\n",
    "        img = img.reshape(-1, self.img_size[0] * self.img_size[1])\n",
    "        hidden = F.relu(self.fc0(img))\n",
    "        hidden = self.fc1(hidden)\n",
    "        # mean of noise, used in normal distribution\n",
    "        noise_map = self.fc20(F.relu(self.fc2(F.relu(hidden))))\n",
    "        mean =  self.fc21(F.relu(noise_map))\n",
    "        # std used in normal distribution\n",
    "        sigma = torch.tensor([[1e-8] for _ in range(BS)]).float()\n",
    "        if USE_CUDA:\n",
    "            sigma = sigma.cuda()\n",
    "        return mean, sigma, noise_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "patient-sunset",
   "metadata": {
    "id": "patient-sunset"
   },
   "outputs": [],
   "source": [
    "class NumNet(nn.Module):\n",
    "    def __init__(self, img_size, out_size = 3):\n",
    "        \"\"\"\n",
    "        Network for learning N, number of letters in a captcha\n",
    "        \"\"\"\n",
    "        super(NumNet, self).__init__()\n",
    "        self.neural_net = nn.Sequential(\n",
    "            nn.Linear(img_size[0] * img_size[1], img_size[0] * img_size[1] * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(img_size[0] * img_size[1] * 2, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, out_size),\n",
    "            nn.LogSoftmax(dim=1))\n",
    "  \n",
    "    def forward(self, img):\n",
    "        img = torch.reshape(img, (img.shape[0], img.shape[1] * img.shape[2]))\n",
    "        prob = self.neural_net(img)\n",
    "        return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "authorized-sender",
   "metadata": {
    "id": "authorized-sender"
   },
   "outputs": [],
   "source": [
    "class InputEmbedding(nn.Module):\n",
    "    def __init__(self, img_size, output_size, MAX_N):\n",
    "        \"\"\"\n",
    "        Network for letters in a captcha, given the noise and number of letters\n",
    "        \"\"\"\n",
    "        super(InputEmbedding, self).__init__()\n",
    "        self.img_size = img_size\n",
    "        \n",
    "        # observe layers\n",
    "        self.nnfc = nn.Linear(img_size[0] * img_size[1], img_size[0] * img_size[1])\n",
    "        self.conv1 = nn.Conv2d(1, 64, 3)\n",
    "        self.conv2 = nn.Conv2d(64, 64, 3)\n",
    "        self.conv3 = nn.Conv2d(64, 64, 3)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.convBN1 = nn.BatchNorm2d(64)\n",
    "        self.convBN2 = nn.BatchNorm2d(64)\n",
    "        self.convBN3 = nn.BatchNorm2d(64)\n",
    "\n",
    "    def forward(self, img, noise_map):\n",
    "        \n",
    "        BATCH_SIZE = img.shape[0]\n",
    "        # feed the noise_map to a linear layer to tune the values\n",
    "        noise_map = F.relu(self.nnfc(noise_map))\n",
    "        noise_map = torch.reshape(noise_map, (BATCH_SIZE, 1, self.img_size[0], self.img_size[1]))\n",
    "    \n",
    "        img = torch.reshape(img, (BATCH_SIZE, 1, self.img_size[0], self.img_size[1]))\n",
    "        # compute the difference between original image and noise map\n",
    "        # i.e. extract the unnoisy image\n",
    "        img = torch.sub(img, noise_map)\n",
    "\n",
    "        img = self.pool(F.relu(self.convBN1(self.conv1(img))))\n",
    "        img = self.pool(F.relu(self.convBN2(self.conv2(img))))\n",
    "        img = self.pool(F.relu(self.convBN3(self.conv3(img))))\n",
    "        \n",
    "        img = torch.reshape(img, (BATCH_SIZE, 1280))\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sound-pollution",
   "metadata": {
    "id": "sound-pollution"
   },
   "outputs": [],
   "source": [
    "# Parallel the networks\n",
    "class CharNetSingle(nn.Module):\n",
    "    def __init__(self, img_size, output_size, MAX_N):\n",
    "        \"\"\"\n",
    "        Network for letters in a captcha, given the noise and number of letters\n",
    "        \"\"\"\n",
    "        super(CharNetSingle, self).__init__()\n",
    "        self.img_size = img_size\n",
    "        self.MAX_N = MAX_N\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        self.conv1 = nn.Conv1d(in_channels=1280 * MAX_N, out_channels=2048 * MAX_N, kernel_size=1, groups=MAX_N)\n",
    "        self.conv2 = nn.Conv1d(in_channels=2048 * MAX_N, out_channels=1024 * MAX_N, kernel_size=1, groups=MAX_N)\n",
    "        self.conv3 = nn.Conv1d(in_channels=1024 * MAX_N, out_channels=1024 * MAX_N, kernel_size=1, groups=MAX_N)\n",
    "        self.conv4 = nn.Conv1d(in_channels=1024 * MAX_N, out_channels=output_size * MAX_N, kernel_size=1, groups=MAX_N)\n",
    "\n",
    "    def forward(self, img_embedded, N):\n",
    "        BATCH_SIZE = img_embedded.shape[0]\n",
    "        \n",
    "        img_embedded = torch.reshape(img_embedded, (img_embedded.shape[0], img_embedded.shape[1], 1)).repeat(1, self.MAX_N, 1)\n",
    "\n",
    "        out = F.relu(self.conv1(img_embedded)) # input shape B x (img_embedded x MAX_N) x 1\n",
    "        out = F.relu(self.conv2(out))\n",
    "        out = F.relu(self.conv3(out))\n",
    "        out = self.conv4(out) # output shape B x (output_size x MAX_N) x 1\n",
    "        \n",
    "        # we only want the first N x output_size outputs\n",
    "        out = out[:, : N * self.output_size]\n",
    "        out = torch.reshape(out, (BATCH_SIZE, N, self.output_size)) # output shape B x N x output_size (10)\n",
    "        out = F.log_softmax(out, dim=2)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "overhead-meditation",
   "metadata": {
    "id": "overhead-meditation"
   },
   "outputs": [],
   "source": [
    "def inference(t, use_cuda=False):\n",
    "    \"\"\"\n",
    "    one epoch of inference (iterate the training set once)\n",
    "    \"\"\"\n",
    "    start = time.time()\n",
    "    loss = 0\n",
    "    length = TrainingSample\n",
    "    loss_group = []\n",
    "    all_batches = make_batches(TrainLoaders)\n",
    "    for i_batch, sample_batched in enumerate(all_batches):\n",
    "        \n",
    "        img = sample_batched[1]\n",
    "        if use_cuda:\n",
    "            img = img.cuda()\n",
    "        IMG = {\"captcha\" : img}\n",
    "        imme_loss = csis.step(observations=IMG)\n",
    "        loss += imme_loss / length\n",
    "\n",
    "    print(\"loss at epoch {} is {}\".format(t, loss), end=\"; \")\n",
    "    print(\"Epoch takes\", round(time.time()- start), \"seconds\")\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "agreed-provincial",
   "metadata": {
    "id": "agreed-provincial"
   },
   "outputs": [],
   "source": [
    "def test(n = 0, use_train=False, verbose=False, use_cuda=False):\n",
    "    \"\"\"\n",
    "    benchmarking performance on customized or training set\n",
    "    \"\"\"\n",
    "    if use_train:\n",
    "        TestLoaders = make_loarders(BATCH_SIZE=1, raw_samples=captcha_generated)\n",
    "    else:\n",
    "        test_captcha_generated = generate_random_captcha(n, save=False)\n",
    "        TestLoaders = make_loarders(BATCH_SIZE=1, raw_samples=test_captcha_generated)\n",
    "    \n",
    "    total_correct = 0\n",
    "    char_correct = 0\n",
    "    total_char = 0\n",
    "    all_batches = make_batches(TestLoaders)\n",
    "    noise_difference = 0\n",
    "    for i_batch, t in enumerate(all_batches):\n",
    "\n",
    "        label = t[0][0]\n",
    "        gt_noise = t[2][0]\n",
    "        img = t[1]\n",
    "\n",
    "        if use_cuda:\n",
    "            img = img.cuda()\n",
    "        \n",
    "        IMG = {\"captcha\" : img}\n",
    "        \n",
    "        posterior = csis.run(observations=IMG)\n",
    "        marginal_num = pyro.infer.EmpiricalMarginal(posterior, \"num_char\")\n",
    "        marginal_noise = pyro.infer.EmpiricalMarginal(posterior, \"noise\")\n",
    "        with torch.no_grad():\n",
    "\n",
    "            N_index = marginal_num()\n",
    "            N = N_index + captchaModel.num_char_domain[0]\n",
    "            noise = captchaModel._map_to_noise_range(marginal_noise()[0])\n",
    "            sampled_chars = []\n",
    "            \n",
    "            # sample characters one by one\n",
    "            for i in range(N):\n",
    "                marginal_char = pyro.infer.EmpiricalMarginal(posterior, \"char_{}\".format(i))()[0]\n",
    "                if use_cuda:\n",
    "                    marginal_char.cpu()\n",
    "                sampled_chars.append(marginal_char)\n",
    "        \n",
    "        chars = \"\"\n",
    "        for i in range(len(sampled_chars)):\n",
    "            c = sampled_chars[i]\n",
    "            chars +=  captchaModel.char_dict[c]\n",
    "        correct = 0\n",
    "        \n",
    "        for p_char, t_char in zip(chars, label):\n",
    "            if p_char == t_char:\n",
    "                correct += 1\n",
    "        noise_difference += abs(float(noise) - float(gt_noise))\n",
    "        if not verbose:\n",
    "            print(\"N_predicted:\", int(N), \"| Actual N:\", len(label), \"| Predicted Noise:\", round(float(noise), 3), \"| Actual Noise:\", round(float(gt_noise), 3), \"| Predicted Text:\", chars, \"| Actual Text:\", label, \"| Correct:\", correct)\n",
    "        if correct == len(label) and int(N) == len(label):\n",
    "            total_correct += 1\n",
    "        char_correct += correct\n",
    "        total_char += len(label)\n",
    "    num_test_samples = i_batch + 1\n",
    "    accuracy = total_correct / num_test_samples\n",
    "    char_accuracy = char_correct / total_char\n",
    "    noise_difference = noise_difference / num_test_samples\n",
    "    print(\"use_train =\", use_train, \"AVG Noise Difference:\", noise_difference, \"Total correct:\", total_correct, \"accuracy:{}/{}=\".format(total_correct, num_test_samples), accuracy, \"char_accuracy:{}/{}=\".format(char_correct, total_char), char_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "occupied-invitation",
   "metadata": {
    "id": "occupied-invitation"
   },
   "outputs": [],
   "source": [
    "def test_cycle(use_cuda):\n",
    "    \n",
    "    # disable dropout\n",
    "    #captchaModel.numNet.eval()\n",
    "    #captchaModel.charNetSingle.eval()\n",
    "    test(use_train=True, verbose=True, use_cuda=use_cuda)\n",
    "    test(1000, use_train=False, verbose=True, use_cuda=use_cuda)\n",
    "    #test(10, use_train=True, verbose=False, use_cuda=use_cuda)\n",
    "    test(10, use_train=False, verbose=False, use_cuda=use_cuda)\n",
    "    # enable dropout\n",
    "    #captchaModel.numNet.train()\n",
    "    #captchaModel.charNetSingle.train()\n",
    "\n",
    "def optimize(start_epoch=1, use_cuda=False):\n",
    "    \"\"\"\n",
    "    Training/Inferencing Stage\n",
    "    \"\"\"\n",
    "    loss_sequence = []\n",
    "    pause = 5\n",
    "    save_pause = 10\n",
    "    print(\"Optimizing...\")\n",
    "    for t in range(start_epoch, num_steps + 1):\n",
    "        L = inference(t, use_cuda)\n",
    "        loss_sequence.append(L)\n",
    "        if (t % pause == 0) and (t > 0):\n",
    "            test_cycle(use_cuda=use_cuda)\n",
    "        if (t % save_pause == 0) and (t > 0):\n",
    "            save_and_download_checkpoints(\"branches-1-no-var-no-tanh_model.pt\", \"branches-1-no-var-no-tanh_optim.pt\", \"branches-1-no-var-no-tanh_param_store.pt\")\n",
    "    plt.plot(loss_sequence)\n",
    "    plt.title(\"loss\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "JtIuFFPvdzKk",
   "metadata": {
    "id": "JtIuFFPvdzKk"
   },
   "outputs": [],
   "source": [
    "# saves the model and optimizer states to disk\n",
    "def save_checkpoint(currentModel, currentOptimzier, save_model, save_opt, save_param_store):\n",
    "    print(\"saving model to %s...\" % save_model)\n",
    "    torch.save(currentModel.state_dict(), save_model)\n",
    "    print(\"saving optimizer states to %s...\" % save_opt)\n",
    "    currentOptimzier.save(save_opt)\n",
    "    print(\"saving pyro pram store states to %s...\" % save_param_store)\n",
    "    pyro.get_param_store().save(save_param_store)\n",
    "    print(\"done saving checkpoints to disk.\")\n",
    "\n",
    "# loads the model and optimizer states from disk\n",
    "def load_checkpoint(myModel, myOptimzer, load_model, load_opt, load_param_store):\n",
    "    pyro.clear_param_store()\n",
    "    print(\"loading model from %s...\" % load_model)\n",
    "    myModel.load_state_dict(torch.load(load_model))\n",
    "    print(\"loading optimizer states from %s...\" % load_opt)\n",
    "    myOptimzer.load(load_opt)\n",
    "    print(\"loading pyro pram store states from %s...\" % load_param_store)\n",
    "    pyro.get_param_store().load(load_param_store)\n",
    "    print(\"done loading states.\")\n",
    "    pyro.module(\"guide\", myModel, update_module_params=True)\n",
    "\n",
    "def save_and_download_checkpoints(save_model, save_opt, save_param_store):\n",
    "    save_checkpoint(captchaModel, optimiser, save_model, save_opt, save_param_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "composite-secretary",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "composite-secretary",
    "outputId": "bcfa2f36-6a17-400c-83c6-943e44d35a54"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/pyro/util.py:282: UserWarning: Found plate statements in guide but not model: {'char_sample_loop'}\n",
      "  warnings.warn(\"Found plate statements in guide but not model: {}\".format(guide_vars - model_vars))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at epoch 1 is 363511868157016.0; Epoch takes 697 seconds\n",
      "loss at epoch 2 is 109616324878086.94; Epoch takes 714 seconds\n",
      "loss at epoch 3 is 72999071999406.86; Epoch takes 699 seconds\n",
      "loss at epoch 4 is 54640734767153.18; Epoch takes 695 seconds\n",
      "loss at epoch 5 is 46373010504967.734; Epoch takes 684 seconds\n",
      "use_train = True AVG Noise Difference: 0.036652434447397374 Total correct: 6572 accuracy:6572/10000= 0.6572 char_accuracy:40930/45021= 0.9091312942848893\n",
      "use_train = False AVG Noise Difference: 0.037805729940913184 Total correct: 657 accuracy:657/1000= 0.657 char_accuracy:4091/4508= 0.9074977817213842\n",
      "N_predicted: 4 | Actual N: 4 | Predicted Noise: 0.178 | Actual Noise: 0.077 | Predicted Text: 1058 | Actual Text: 1058 | Correct: 4\n",
      "N_predicted: 5 | Actual N: 5 | Predicted Noise: 0.65 | Actual Noise: 0.662 | Predicted Text: 43687 | Actual Text: 43687 | Correct: 5\n",
      "N_predicted: 4 | Actual N: 4 | Predicted Noise: 0.341 | Actual Noise: 0.368 | Predicted Text: 7025 | Actual Text: 7025 | Correct: 4\n",
      "N_predicted: 5 | Actual N: 5 | Predicted Noise: 0.501 | Actual Noise: 0.523 | Predicted Text: 87314 | Actual Text: 87314 | Correct: 5\n",
      "N_predicted: 5 | Actual N: 5 | Predicted Noise: 0.368 | Actual Noise: 0.379 | Predicted Text: 56726 | Actual Text: 56726 | Correct: 5\n",
      "N_predicted: 4 | Actual N: 4 | Predicted Noise: 0.218 | Actual Noise: 0.185 | Predicted Text: 5191 | Actual Text: 5191 | Correct: 4\n",
      "N_predicted: 5 | Actual N: 5 | Predicted Noise: 0.209 | Actual Noise: 0.165 | Predicted Text: 32971 | Actual Text: 32271 | Correct: 4\n",
      "N_predicted: 4 | Actual N: 4 | Predicted Noise: 0.226 | Actual Noise: 0.197 | Predicted Text: 2390 | Actual Text: 2390 | Correct: 4\n",
      "N_predicted: 4 | Actual N: 4 | Predicted Noise: 0.755 | Actual Noise: 0.761 | Predicted Text: 3907 | Actual Text: 3907 | Correct: 4\n",
      "N_predicted: 4 | Actual N: 4 | Predicted Noise: 0.583 | Actual Noise: 0.607 | Predicted Text: 2733 | Actual Text: 2733 | Correct: 4\n",
      "use_train = False AVG Noise Difference: 0.030730315750317306 Total correct: 9 accuracy:9/10= 0.9 char_accuracy:43/44= 0.9772727272727273\n",
      "loss at epoch 6 is 42127484786940.37; Epoch takes 686 seconds\n",
      "loss at epoch 7 is 39786271412453.39; Epoch takes 688 seconds\n",
      "loss at epoch 8 is 37195034134852.39; Epoch takes 690 seconds\n",
      "loss at epoch 9 is 35727766576221.4; Epoch takes 693 seconds\n",
      "loss at epoch 10 is 33538795317362.65; Epoch takes 686 seconds\n",
      "use_train = True AVG Noise Difference: 0.037377994631343045 Total correct: 8183 accuracy:8183/10000= 0.8183 char_accuracy:43025/45021= 0.9556651340485551\n",
      "use_train = False AVG Noise Difference: 0.038672684203579735 Total correct: 812 accuracy:812/1000= 0.812 char_accuracy:4290/4499= 0.9535452322738386\n",
      "N_predicted: 5 | Actual N: 5 | Predicted Noise: 0.822 | Actual Noise: 0.845 | Predicted Text: 24766 | Actual Text: 24766 | Correct: 5\n",
      "N_predicted: 5 | Actual N: 5 | Predicted Noise: 0.731 | Actual Noise: 0.746 | Predicted Text: 89163 | Actual Text: 89163 | Correct: 5\n",
      "N_predicted: 5 | Actual N: 5 | Predicted Noise: 0.872 | Actual Noise: 0.923 | Predicted Text: 99450 | Actual Text: 99450 | Correct: 5\n",
      "N_predicted: 4 | Actual N: 4 | Predicted Noise: 0.506 | Actual Noise: 0.478 | Predicted Text: 3793 | Actual Text: 3793 | Correct: 4\n",
      "N_predicted: 5 | Actual N: 5 | Predicted Noise: 0.843 | Actual Noise: 0.869 | Predicted Text: 87499 | Actual Text: 87469 | Correct: 4\n",
      "N_predicted: 4 | Actual N: 4 | Predicted Noise: 0.762 | Actual Noise: 0.777 | Predicted Text: 3375 | Actual Text: 3375 | Correct: 4\n",
      "N_predicted: 4 | Actual N: 4 | Predicted Noise: 0.598 | Actual Noise: 0.572 | Predicted Text: 4349 | Actual Text: 4349 | Correct: 4\n",
      "N_predicted: 4 | Actual N: 4 | Predicted Noise: 0.298 | Actual Noise: 0.274 | Predicted Text: 3830 | Actual Text: 3830 | Correct: 4\n",
      "N_predicted: 5 | Actual N: 5 | Predicted Noise: 0.286 | Actual Noise: 0.274 | Predicted Text: 43247 | Actual Text: 43247 | Correct: 5\n",
      "N_predicted: 5 | Actual N: 5 | Predicted Noise: 0.575 | Actual Noise: 0.601 | Predicted Text: 67106 | Actual Text: 67106 | Correct: 5\n",
      "use_train = False AVG Noise Difference: 0.02439712789411582 Total correct: 9 accuracy:9/10= 0.9 char_accuracy:45/46= 0.9782608695652174\n",
      "saving model to branches-1-no-var-no-tanh_model.pt...\n",
      "saving optimizer states to branches-1-no-var-no-tanh_optim.pt...\n",
      "saving pyro pram store states to branches-1-no-var-no-tanh_param_store.pt...\n",
      "done saving checkpoints to disk.\n",
      "loss at epoch 11 is 31900663665131.523; Epoch takes 681 seconds\n",
      "loss at epoch 12 is 29758236640359.234; Epoch takes 685 seconds\n",
      "loss at epoch 13 is 29687340988314.438; Epoch takes 686 seconds\n",
      "loss at epoch 14 is 28181938921301.613; Epoch takes 677 seconds\n",
      "loss at epoch 15 is 27533161773727.7; Epoch takes 674 seconds\n",
      "use_train = True AVG Noise Difference: 0.03176314768596146 Total correct: 8622 accuracy:8622/10000= 0.8622 char_accuracy:43521/45021= 0.966682214966349\n",
      "use_train = False AVG Noise Difference: 0.03281066102739952 Total correct: 867 accuracy:867/1000= 0.867 char_accuracy:4347/4485= 0.9692307692307692\n",
      "N_predicted: 4 | Actual N: 4 | Predicted Noise: 0.707 | Actual Noise: 0.738 | Predicted Text: 3213 | Actual Text: 3213 | Correct: 4\n",
      "N_predicted: 4 | Actual N: 4 | Predicted Noise: 0.159 | Actual Noise: 0.016 | Predicted Text: 3976 | Actual Text: 3976 | Correct: 4\n",
      "N_predicted: 5 | Actual N: 5 | Predicted Noise: 0.214 | Actual Noise: 0.187 | Predicted Text: 97951 | Actual Text: 97951 | Correct: 5\n",
      "N_predicted: 4 | Actual N: 4 | Predicted Noise: 0.16 | Actual Noise: 0.033 | Predicted Text: 0672 | Actual Text: 0672 | Correct: 4\n",
      "N_predicted: 4 | Actual N: 4 | Predicted Noise: 0.159 | Actual Noise: 0.039 | Predicted Text: 1927 | Actual Text: 1927 | Correct: 4\n",
      "N_predicted: 4 | Actual N: 4 | Predicted Noise: 0.829 | Actual Noise: 0.865 | Predicted Text: 1227 | Actual Text: 1227 | Correct: 4\n",
      "N_predicted: 4 | Actual N: 4 | Predicted Noise: 0.828 | Actual Noise: 0.838 | Predicted Text: 7450 | Actual Text: 7450 | Correct: 4\n",
      "N_predicted: 4 | Actual N: 4 | Predicted Noise: 0.581 | Actual Noise: 0.597 | Predicted Text: 3305 | Actual Text: 3305 | Correct: 4\n",
      "N_predicted: 4 | Actual N: 4 | Predicted Noise: 0.201 | Actual Noise: 0.141 | Predicted Text: 9363 | Actual Text: 9363 | Correct: 4\n",
      "N_predicted: 4 | Actual N: 4 | Predicted Noise: 0.862 | Actual Noise: 0.899 | Predicted Text: 5082 | Actual Text: 5982 | Correct: 3\n",
      "use_train = False AVG Noise Difference: 0.060563490263144816 Total correct: 9 accuracy:9/10= 0.9 char_accuracy:40/41= 0.975609756097561\n",
      "loss at epoch 16 is 26364456553703.004; Epoch takes 679 seconds\n",
      "loss at epoch 17 is 25674877963822.7; Epoch takes 691 seconds\n",
      "loss at epoch 18 is 25452026979339.477; Epoch takes 682 seconds\n"
     ]
    }
   ],
   "source": [
    "class CaptchaModel(nn.Module):\n",
    "    \"\"\"\n",
    "    network, model and guide wrapper class\n",
    "    \"\"\"\n",
    "    def __init__(self, use_cuda=False):\n",
    "        super().__init__()\n",
    "        self.num_char_domain = torch.arange(MIN_N, MAX_N + 1)\n",
    "        if use_cuda:\n",
    "            self.num_char_domain = self.num_char_domain.cuda()\n",
    "\n",
    "        self.numNet = NumNet((captchaHeight, captchaWidth), len(self.num_char_domain))\n",
    "        self.noiseNet = NoiseNet((captchaHeight, captchaWidth), 1)\n",
    "        self.char_dict = char_dict # letter dictionary\n",
    "        self.rnn_hidden_size = 512\n",
    "        self.rnn_num_layer = 2\n",
    "        self.charNet = CharNetSingle((captchaHeight, captchaWidth), len(self.char_dict), MAX_N)\n",
    "        self.inputEmbedding = InputEmbedding((captchaHeight, captchaWidth), len(self.char_dict), max(self.num_char_domain))\n",
    "        self.noise_constraint = torch.distributions.constraints.interval(MIN_NOISE, MAX_NOISE)\n",
    "        if use_cuda:\n",
    "            self.cuda()\n",
    "        self.use_cuda = use_cuda\n",
    "    \n",
    "    def _map_to_noise_range(self, input):\n",
    "        \"\"\"\n",
    "        map input number to the valid noise range\n",
    "        \"\"\"\n",
    "        input = torch.distributions.transform_to(self.noise_constraint)(input)\n",
    "        return input\n",
    "\n",
    "    def guide(self, observations={\"captcha\": torch.rand(1, captchaHeight, captchaWidth)}):\n",
    "        pyro.module(\"guide\", self)\n",
    "        img = observations[\"captcha\"].float()\n",
    "        \n",
    "        # posterior to the number of letters\n",
    "        prob = self.numNet(img)\n",
    "        prob = torch.mean(prob, dim=0)\n",
    "        N_index = pyro.sample(\"num_char\", dist.Categorical(prob))\n",
    "        N_index = torch.add(N_index, self.num_char_domain[0])\n",
    "\n",
    "        with pyro.plate(\"data\", img.shape[0]):\n",
    "            \n",
    "            # posterior to the noise\n",
    "            noise_mean, noise_sig, noise_map = self.noiseNet(img)\n",
    "            noise_batch = pyro.sample(\"noise\", dist.Normal(noise_mean, noise_sig).to_event(1))\n",
    "            noise_batch = self._map_to_noise_range(noise_batch)\n",
    "            \n",
    "            # compute the input embedding first (CNNs)\n",
    "            input_emb = self.inputEmbedding(img, noise_map)\n",
    "\n",
    "            charP = self.charNet(input_emb, int(N_index))\n",
    "            # charP.shape = B x N_index x len(char_dict)\n",
    "            \n",
    "            for i in pyro.plate(\"char_sample_loop\", int(N_index)):\n",
    "                c = pyro.sample(\"char_{}\".format(i), dist.Categorical(charP[:,i,:]).to_event(0))\n",
    "\n",
    "    def model(self, observations={\"captcha\": torch.rand(1, captchaHeight, captchaWidth)}):\n",
    "        \n",
    "        BS = observations[\"captcha\"].shape[0]\n",
    "        \n",
    "        num_p = torch.tensor(1 / len(self.num_char_domain)).repeat(len(self.num_char_domain))\n",
    "        \n",
    "        if self.use_cuda:\n",
    "            num_p = num_p.cuda()\n",
    "        \n",
    "        # sample the number of characters\n",
    "        N_index = pyro.sample(\"num_char\", dist.Categorical(num_p))\n",
    "        N_index = torch.add(N_index,  self.num_char_domain[0])\n",
    "        \n",
    "        with pyro.plate(\"data\", BS):\n",
    "            \n",
    "            noise_mean = torch.tensor((MAX_NOISE - MIN_NOISE) / 2).repeat((BS, 1))\n",
    "            noise_sig = torch.tensor(0.5).repeat((BS, 1))\n",
    "\n",
    "            if self.use_cuda:\n",
    "                noise_mean = noise_mean.cuda()\n",
    "                noise_sig = noise_sig.cuda()\n",
    "\n",
    "            # sample the noise\n",
    "            noise_batch = pyro.sample(\"noise\", dist.Normal(noise_mean, noise_sig).to_event(1))\n",
    "            noise_batch = self._map_to_noise_range(noise_batch)\n",
    "            \n",
    "            # assume the user don't know the independence and sample in a loop\n",
    "            sampled_c = []\n",
    "            for i in range(int(N_index)):\n",
    "                num_c_i = torch.tensor(1 / len(self.char_dict)).repeat((BS, len(self.char_dict)))\n",
    "                if self.use_cuda:\n",
    "                    num_c_i = num_c_i.cuda()\n",
    "                c_i = pyro.sample(\"char_{}\".format(i), dist.Categorical(num_c_i).to_event(0))\n",
    "                sampled_c.append(c_i)\n",
    "            \n",
    "            # sample characters\n",
    "            rendered_images = []\n",
    "            \n",
    "            for i in range(sampled_c[0].shape[0]):\n",
    "                chars = \"\"\n",
    "                for j in range(N_index):\n",
    "                    chars += self.char_dict[sampled_c[j][i]]\n",
    "        \n",
    "                rendered_image = render_image(chars, noise=float(noise_batch[i]), use_cuda=self.use_cuda)\n",
    "                rendered_images.append(rendered_image)\n",
    "                \n",
    "        rendered_images = torch.stack(rendered_images)\n",
    "        sigma = torch.tensor(0.000001)\n",
    "        if self.use_cuda:\n",
    "                sigma = sigma.cuda()\n",
    "\n",
    "        pyro.sample(\"captcha\", dist.Normal(rendered_images, sigma).to_event(2), obs=observations[\"captcha\"])\n",
    "\n",
    "captchaModel = CaptchaModel(USE_CUDA)\n",
    "\n",
    "optimiser = pyro.optim.Adam({'lr': 5e-5})\n",
    "csis = pyro.infer.CSIS(captchaModel.model, captchaModel.guide, optimiser, num_inference_samples=1)\n",
    "\n",
    "\n",
    "optimize(1, USE_CUDA)\n",
    "test_cycle(USE_CUDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "swiss-sister",
   "metadata": {
    "id": "swiss-sister"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "genetic-difficulty",
   "metadata": {
    "id": "genetic-difficulty"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copy_of_Copy_of_Captcha_CSIS_CUDA_noise_depend_input_embedding_parallel_guide_for_in_model_more.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
