{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "standard-happening",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "hid_orderings [1, 2, 2, 1, 2, 2, 1, 0, 1, 0, 2, 1, 2, 0, 0, 0] size 16\n",
      "expanded_input_ordering [0, 0, 0, 0, 0, 0, 0, 0, 1] size 9\n",
      "expanded_output_ordering [2, 2] size 2\n",
      "hid_orderings [1, 2, 1, 3, 2, 0, 0, 2, 0, 3, 0, 3, 0, 4, 2, 6, 1, 3, 5, 2, 2, 4, 6, 6, 2, 5, 4, 2, 1, 0, 6, 0] size 32\n",
      "expanded_input_ordering [0, 0, 0, 0, 0, 0, 0, 0, 1, 2] size 10\n",
      "expanded_output_ordering [4, 6, 4, 6] size 4\n",
      "hid_orderings [7, 8, 2, 9, 13, 11, 5, 1, 14, 7, 9, 12, 5, 0, 2, 10, 5, 13, 11, 14, 10, 4, 11, 0, 7, 13, 7, 2, 7, 8, 5, 13, 5, 4, 7, 3, 4, 5, 14, 1, 2, 2, 14, 13, 2, 8, 1, 4, 5, 8, 13, 12, 8, 11, 7, 10, 11, 3, 12, 8, 7, 5, 3, 7] size 64\n",
      "expanded_input_ordering [0, 1, 1, 1, 1, 1, 1, 1, 1, 2, 3] size 11\n",
      "expanded_output_ordering [7, 11, 8, 7, 11, 8] size 6\n",
      "hid_orderings [7, 39, 56, 49, 38, 31, 43, 7, 3, 54, 22, 42, 26, 22, 48, 21, 47, 51, 11, 61, 3, 26, 31, 52, 11, 35, 23, 58, 7, 16, 51, 20, 59, 3, 24, 6, 36, 60, 20, 40, 16, 27, 36, 15, 43, 31, 57, 17, 60, 11, 1, 41, 46, 30, 2, 62, 8, 54, 56, 13, 54, 29, 56, 55, 48, 18, 36, 61, 20, 24, 43, 3, 15, 10, 26, 26, 38, 31, 10, 2, 33, 48, 58, 52, 19, 54, 29, 17, 5, 27, 34, 17, 12, 59, 4, 59, 25, 49, 6, 35, 47, 27, 22, 31, 12, 13, 15, 2, 57, 4, 58, 0, 41, 34, 2, 59, 46, 55, 27, 18, 11, 7, 42, 51, 13, 1, 22, 9, 14, 13, 41, 27, 35, 24, 61, 31, 2, 8, 46, 57, 7, 28, 54, 34, 1, 32, 18, 13, 59, 6, 52, 47, 37, 20, 5, 42, 60, 11, 7, 29, 53, 0, 49, 5, 2, 9, 28, 59, 41, 3, 9, 29, 5, 42, 22, 60, 51, 51, 14, 27, 42, 5, 0, 20, 25, 49, 51, 3, 4, 58, 18, 7, 49, 49, 38, 48, 54, 53, 16, 26, 62, 23, 39, 53, 55, 37, 22, 43, 0, 22, 47, 40, 5, 45, 38, 28, 6, 37, 46, 12, 39, 4, 12, 22, 49, 17, 56, 61, 10, 1, 45, 48, 56, 42, 49, 23, 6, 62, 38, 17, 56, 4, 3, 16, 8, 25, 39, 32, 34, 7, 33, 61, 20, 50, 40, 48] size 256\n",
      "expanded_input_ordering [0, 1, 2, 3, 4, 4, 4, 4, 4, 4, 4, 4, 5] size 13\n",
      "expanded_output_ordering [18, 16, 32, 30, 18, 16, 32, 30] size 8\n",
      "hid_orderings [41, 27, 14, 5, 7, 6, 48, 52, 51, 55, 22, 42, 51, 8, 0, 13, 57, 50, 8, 53, 11, 16, 10, 55, 52, 4, 1, 49, 24, 35, 40, 34, 11, 19, 37, 49, 21, 1, 9, 17, 60, 26, 31, 26, 54, 50, 9, 41, 34, 56, 3, 36, 11, 35, 51, 27, 33, 23, 59, 54, 5, 2, 33, 19, 1, 42, 9, 40, 6, 44, 52, 50, 55, 38, 21, 11, 41, 61, 41, 6, 2, 9, 50, 8, 44, 36, 22, 26, 16, 25, 11, 33, 41, 34, 62, 58, 56, 38, 9, 47, 59, 37, 28, 41, 39, 52, 30, 23, 50, 29, 40, 17, 33, 35, 36, 49, 21, 10, 23, 36, 30, 23, 26, 11, 6, 41, 4, 36, 61, 31, 44, 48, 30, 33, 14, 39, 33, 21, 59, 39, 26, 56, 48, 61, 21, 30, 14, 45, 52, 43, 44, 9, 58, 8, 50, 10, 43, 46, 11, 35, 15, 60, 11, 61, 31, 0, 9, 22, 42, 40, 14, 26, 53, 29, 20, 40, 54, 43, 61, 27, 0, 2, 61, 23, 49, 61, 34, 42, 35, 45, 51, 43, 41, 2, 7, 28, 51, 52, 26, 52, 21, 40, 7, 60, 22, 19, 4, 12, 2, 35, 61, 0, 30, 26, 44, 4, 11, 21, 20, 26, 30, 24, 22, 59, 6, 16, 43, 23, 11, 54, 26, 40, 58, 58, 27, 28, 2, 3, 58, 59, 24, 52, 46, 23, 3, 38, 7, 27, 31, 2, 12, 18, 44, 9, 38, 22] size 256\n",
      "expanded_input_ordering [0, 1, 2, 3, 4, 5, 5, 5, 5, 5, 5, 5, 5] size 13\n",
      "expanded_output_ordering [30, 35, 17, 30, 35, 17] size 6\n",
      "hid_orderings [3, 6, 4, 5, 1, 1, 3, 1, 2, 5, 1, 3, 2, 4, 3, 6, 3, 6, 0, 2, 3, 0, 6, 5, 2, 3, 3, 1, 0, 4, 5, 3] size 32\n",
      "expanded_input_ordering [0, 0, 0, 0, 0, 0, 0, 0, 1, 2] size 10\n",
      "expanded_output_ordering [6, 6] size 2\n",
      "number of levels: 6\n",
      "input_levels [['h', 'r'], ['h', 'r', 'z2'], ['z2', 'h', 'z1', 'y4'], ['y4', 'z1', 'y2', 'y3', 'h', 'x8'], ['x6', 'x4', 'y1', 'y2', 'y3', 'h'], ['h', 'y1', 'x2']]\n",
      "out_levels [['z2'], ['y4', 'z1'], ['y2', 'y3', 'x8'], ['x6', 'x4', 'y1', 'x7'], ['x5', 'x3', 'x2'], ['x1']]\n",
      "[Step 10/200] Immediate Loss: 7.73021 Accumlated Loss: 12.15332 Duration: 2.633\n",
      "[Step 20/200] Immediate Loss: 7.16565 Accumlated Loss: 7.40011 Duration: 2.587\n",
      "[Step 30/200] Immediate Loss: 7.08254 Accumlated Loss: 7.03757 Duration: 2.596\n",
      "[Step 40/200] Immediate Loss: 6.83297 Accumlated Loss: 6.98378 Duration: 2.672\n",
      "[Step 50/200] Immediate Loss: 6.86551 Accumlated Loss: 6.91275 Duration: 2.600\n",
      "[Step 60/200] Immediate Loss: 6.93487 Accumlated Loss: 6.87233 Duration: 2.598\n",
      "[Step 70/200] Immediate Loss: 6.71809 Accumlated Loss: 6.81451 Duration: 2.657\n",
      "[Step 80/200] Immediate Loss: 6.80281 Accumlated Loss: 6.88790 Duration: 2.612\n",
      "[Step 90/200] Immediate Loss: 6.90435 Accumlated Loss: 6.78826 Duration: 2.664\n",
      "[Step 100/200] Immediate Loss: 6.73903 Accumlated Loss: 6.83094 Duration: 2.606\n",
      "[Step 110/200] Immediate Loss: 6.89314 Accumlated Loss: 6.80484 Duration: 2.614\n",
      "[Step 120/200] Immediate Loss: 6.80861 Accumlated Loss: 6.74317 Duration: 2.604\n",
      "[Step 130/200] Immediate Loss: 6.80884 Accumlated Loss: 6.77493 Duration: 2.629\n",
      "[Step 140/200] Immediate Loss: 6.83799 Accumlated Loss: 6.81526 Duration: 2.629\n",
      "[Step 150/200] Immediate Loss: 6.75561 Accumlated Loss: 6.78899 Duration: 2.585\n",
      "[Step 160/200] Immediate Loss: 6.70648 Accumlated Loss: 6.76897 Duration: 2.611\n",
      "[Step 170/200] Immediate Loss: 6.71425 Accumlated Loss: 6.77154 Duration: 2.627\n",
      "[Step 180/200] Immediate Loss: 6.73846 Accumlated Loss: 6.73665 Duration: 2.650\n",
      "[Step 190/200] Immediate Loss: 6.62496 Accumlated Loss: 6.73855 Duration: 2.750\n",
      "[Step 200/200] Immediate Loss: 6.67873 Accumlated Loss: 6.73655 Duration: 2.694\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pyro\n",
    "import pyro.contrib.examples.polyphonic_data_loader as poly\n",
    "import pyro.distributions as dist\n",
    "from pyro.infer import SVI, Trace_ELBO\n",
    "from pyro.optim import Adam\n",
    "from matplotlib import pyplot as plt\n",
    "from made import MADE\n",
    "from full_made import FullMade\n",
    "from pyro.infer.autoguide import AutoDiagonalNormal, AutoNormal, AutoMultivariateNormal\n",
    "\n",
    "random.seed(123)\n",
    "torch.manual_seed(123)\n",
    "\n",
    "# NN used for p(x | y)\n",
    "class simpleNN(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden=32, out_size=1, t=\"normal\", out_non_linear=None):\n",
    "        super().__init__()\n",
    "        self.t = t\n",
    "        self.out_non_linear = out_non_linear\n",
    "        self.hiddeen_layer = nn.Linear(input_size, hidden)\n",
    "        if t == \"normal\":\n",
    "            self.loc_layer = nn.Linear(hidden, out_size)\n",
    "            self.std_layer = nn.Linear(hidden, out_size)\n",
    "            self.softplus = nn.Softplus()\n",
    "        elif t == \"bern\":\n",
    "            self.prob_layer = nn.Linear(hidden, out_size)\n",
    "        elif t == \"mlp\":\n",
    "            self.out_layer = nn.Linear(hidden, out_size)\n",
    "        \n",
    "    def forward(self, x_list):\n",
    "        for i in range(len(x_list)):\n",
    "            if x_list[i].dim() == 0:\n",
    "                x_list[i] = torch.unsqueeze(x_list[i], dim=0)\n",
    "        input_x = torch.cat(x_list)\n",
    "        hid = F.relu(self.hiddeen_layer(input_x))\n",
    "        # return loc, std\n",
    "        if self.t == \"normal\":\n",
    "            return self.loc_layer(hid), self.softplus(self.std_layer(hid))\n",
    "        elif self.t == \"bern\":\n",
    "            return torch.sigmoid(self.prob_layer(hid))\n",
    "        else:\n",
    "            if self.out_non_linear == \"tanh\":\n",
    "                return torch.tanh(self.out_layer(hid))\n",
    "            else:\n",
    "                return self.out_layer(hid)\n",
    "\n",
    "class Experiment(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # guide full_made\n",
    "        self.hidden_size_full_made = 8\n",
    "        input_dim_dict = {\n",
    "            \"h\" : self.hidden_size_full_made,\n",
    "            \"r\" : 1\n",
    "        }\n",
    "        var_dim_dict = {\n",
    "            \"x1\" : 1,\n",
    "            \"x2\" : 1,\n",
    "            \"x3\" : 1,\n",
    "            \"x4\" : 1,\n",
    "            \"x5\" : 1,\n",
    "            \"x6\" : 1,\n",
    "            \"x7\" : 1,\n",
    "            \"x8\" : 1,\n",
    "            \"y1\" : 1,\n",
    "            \"y2\" : 1,\n",
    "            \"y3\" : 1,\n",
    "            \"y4\" : 1,\n",
    "            \"z1\" : 1,\n",
    "            \"z2\" : 1\n",
    "        }\n",
    "        dependency_dict = {\n",
    "            \"z2\" : [\"h\", \"r\"],\n",
    "            \"z1\" : [\"h\", \"r\", \"z2\"],\n",
    "            \"y4\" : [\"h\", \"z2\"],\n",
    "            \"y3\" : [\"h\", \"z2\", \"y4\"],\n",
    "            \"y2\" : [\"h\", \"z1\"],\n",
    "            \"y1\" : [\"h\", \"z1\", \"y2\"],\n",
    "            \"x8\" : [\"h\", \"y4\"],\n",
    "            \"x7\" : [\"h\", \"y4\", \"x8\"],\n",
    "            \"x6\" : [\"h\", \"y3\"],\n",
    "            \"x5\" : [\"h\", \"y3\", \"x6\"],\n",
    "            \"x4\" : [\"h\", \"y2\"],\n",
    "            \"x3\" : [\"h\", \"y2\", \"x4\"],\n",
    "            \"x2\" : [\"h\", \"y1\"],\n",
    "            \"x1\" : [\"h\", \"y1\", \"x2\"]\n",
    "        }\n",
    "        hidden_sizes = None # not used so far\n",
    "        self.full_made = FullMade(input_dim_dict, hidden_sizes, dependency_dict, var_dim_dict)\n",
    "        self.h0_full_made = nn.Parameter(torch.zeros(self.hidden_size_full_made))\n",
    "        self.hid_net_full_made = simpleNN(self.hidden_size_full_made + 8 + 4 + 2, out_size = self.hidden_size_full_made, t = \"mlp\")\n",
    "        \n",
    "    def model(self, n, obs):\n",
    "        def tree_model(i, mu):\n",
    "            x1 = pyro.sample(f\"x1{i}\", dist.Normal(mu, 1.0))\n",
    "            x2 = pyro.sample(f\"x2{i}\", dist.Normal(mu, 1.0))\n",
    "            x3 = pyro.sample(f\"x3{i}\", dist.Normal(mu, 1.0))\n",
    "            x4 = pyro.sample(f\"x4{i}\", dist.Normal(mu, 1.0))\n",
    "            x5 = pyro.sample(f\"x5{i}\", dist.Normal(mu, 1.0))\n",
    "            x6 = pyro.sample(f\"x6{i}\", dist.Normal(mu, 1.0))\n",
    "            x7 = pyro.sample(f\"x7{i}\", dist.Normal(mu, 1.0))\n",
    "            x8 = pyro.sample(f\"x8{i}\", dist.Normal(mu, 1.0))\n",
    "            y1 = pyro.sample(f\"y1{i}\", dist.Normal(x1+x2, 1.0))\n",
    "            y2 = pyro.sample(f\"y2{i}\", dist.Normal(x3+x4, 1.0))\n",
    "            y3 = pyro.sample(f\"y3{i}\", dist.Normal(x5+x6, 1.0))\n",
    "            y4 = pyro.sample(f\"y4{i}\", dist.Normal(x7+x8, 1.0))\n",
    "            z1 = pyro.sample(f\"z1{i}\", dist.Normal(y1+y2, 1.0))\n",
    "            z2 = pyro.sample(f\"z2{i}\", dist.Normal(y3+y4, 1.0))\n",
    "            return z1 + z2  \n",
    "            \n",
    "        pyro.module(\"model\", self)\n",
    "        mu = 0\n",
    "        for i in range(n):\n",
    "            mu = tree_model(i, mu)\n",
    "        \n",
    "        pyro.sample(\"obs\", dist.Normal(mu, 1.0), obs=obs)\n",
    "    \n",
    "    # reverse order of guide 6\n",
    "    def guide_full_made(self, n, obs):\n",
    "        def tree_guide(i, hid, last=False):\n",
    "            # has to assume all interations are identical\n",
    "            input_made = {\n",
    "                \"h\" : hid,\n",
    "                \"r\" : obs\n",
    "            }\n",
    "            if not last: \n",
    "                input_made[\"r\"] = torch.tensor([0.])\n",
    "            output_dict = self.full_made(input_made, i)\n",
    "            input_hid = [hid]\n",
    "            input_hid.extend(output_dict.values())\n",
    "            \n",
    "            # feed all outputs to update hid\n",
    "            hid = self.hid_net_full_made(input_hid)\n",
    "\n",
    "            return hid\n",
    "            \n",
    "        pyro.module(\"model\", self)\n",
    "        hid = self.h0_full_made\n",
    "        for i in range(n-1, -1, -1):\n",
    "            hid = tree_guide(i, hid, i==n-1)\n",
    "\n",
    "def generate_data():\n",
    "    \n",
    "    n_min = 2\n",
    "    n_max = 4\n",
    "    n = random.randint(n_min, n_max)\n",
    "    mu = 0\n",
    "    x_len = 8\n",
    "    for i in range(n):\n",
    "        x_noise = torch.randn(x_len) / 4\n",
    "        x_mean = torch.zeros(x_len) + mu\n",
    "        xs = torch.normal(x_mean, 1) + x_noise\n",
    "        ys = []\n",
    "        j = 0\n",
    "        while j < len(xs):\n",
    "            y = dist.Normal(xs[j] + xs[j+1], 2).sample()\n",
    "            ys.append(y)\n",
    "            j +=2\n",
    "        \n",
    "        zs = []\n",
    "        j = 0\n",
    "        while j < len(ys):\n",
    "            z = dist.Normal(ys[j] + ys[j+1], 1.5).sample()\n",
    "            zs.append(z)\n",
    "            j +=2\n",
    "        \n",
    "        \n",
    "        mu = dist.Normal(zs[0] + zs[1], 1).sample() / 10\n",
    "        \n",
    "    return n, mu\n",
    "    \n",
    "data = []\n",
    "num_data = 100\n",
    "for _ in range(num_data):\n",
    "    data.append(generate_data())\n",
    "\n",
    "print(len(data))\n",
    "experiment = Experiment()\n",
    "adam_params = {\"lr\": 0.001, \"betas\": (0.95, 0.999)}\n",
    "optimizer = Adam(adam_params)\n",
    "guide = experiment.guide_full_made # guide_1\n",
    "\n",
    "svi = SVI(experiment.model, guide, optimizer, loss=Trace_ELBO())\n",
    "\n",
    "n_steps = 200\n",
    "log_interval = 10\n",
    "# do gradient steps\n",
    "loss = 0\n",
    "loss_track = []\n",
    "for step in range(1, n_steps + 1):\n",
    "    imme_loss = 0\n",
    "    start = time.time()\n",
    "    for n, obs in data:\n",
    "        imme_loss += svi.step(n, obs.unsqueeze(0)) / num_data\n",
    "        \n",
    "    loss_track.append(imme_loss)\n",
    "    loss += imme_loss / log_interval\n",
    "    \n",
    "    if step % log_interval == 0:\n",
    "        print(\"[Step {}/{}] Immediate Loss: {:.5f} Accumlated Loss: {:.5f} Duration: {:.3f}\".format(step, n_steps, imme_loss, loss, time.time() - start))\n",
    "        loss = 0\n",
    "    \n",
    "plt.plot(loss_track)\n",
    "plt.title(\"Loss\")\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "laden-spine",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adopted-difference",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sharp-egypt",
   "metadata": {},
   "outputs": [],
   "source": [
    "# full made\n",
    "hidden_size = (|all_level_sets| + 1) * 4 \n",
    "[Step 10/200] Immediate Loss: 7.73021 Accumlated Loss: 12.15332 Duration: 2.633\n",
    "[Step 20/200] Immediate Loss: 7.16565 Accumlated Loss: 7.40011 Duration: 2.587\n",
    "[Step 30/200] Immediate Loss: 7.08254 Accumlated Loss: 7.03757 Duration: 2.596\n",
    "[Step 40/200] Immediate Loss: 6.83297 Accumlated Loss: 6.98378 Duration: 2.672\n",
    "[Step 50/200] Immediate Loss: 6.86551 Accumlated Loss: 6.91275 Duration: 2.600\n",
    "[Step 60/200] Immediate Loss: 6.93487 Accumlated Loss: 6.87233 Duration: 2.598\n",
    "[Step 70/200] Immediate Loss: 6.71809 Accumlated Loss: 6.81451 Duration: 2.657\n",
    "[Step 80/200] Immediate Loss: 6.80281 Accumlated Loss: 6.88790 Duration: 2.612\n",
    "[Step 90/200] Immediate Loss: 6.90435 Accumlated Loss: 6.78826 Duration: 2.664\n",
    "[Step 100/200] Immediate Loss: 6.73903 Accumlated Loss: 6.83094 Duration: 2.606\n",
    "[Step 110/200] Immediate Loss: 6.89314 Accumlated Loss: 6.80484 Duration: 2.614\n",
    "[Step 120/200] Immediate Loss: 6.80861 Accumlated Loss: 6.74317 Duration: 2.604\n",
    "[Step 130/200] Immediate Loss: 6.80884 Accumlated Loss: 6.77493 Duration: 2.629\n",
    "[Step 140/200] Immediate Loss: 6.83799 Accumlated Loss: 6.81526 Duration: 2.629\n",
    "[Step 150/200] Immediate Loss: 6.75561 Accumlated Loss: 6.78899 Duration: 2.585\n",
    "[Step 160/200] Immediate Loss: 6.70648 Accumlated Loss: 6.76897 Duration: 2.611\n",
    "[Step 170/200] Immediate Loss: 6.71425 Accumlated Loss: 6.77154 Duration: 2.627\n",
    "[Step 180/200] Immediate Loss: 6.73846 Accumlated Loss: 6.73665 Duration: 2.650\n",
    "[Step 190/200] Immediate Loss: 6.62496 Accumlated Loss: 6.73855 Duration: 2.750\n",
    "[Step 200/200] Immediate Loss: 6.67873 Accumlated Loss: 6.73655 Duration: 2.694"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "mexican-nightlife",
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_made\n",
    "hidden_size = 100\n",
    "[Step 10/100] Immediate Loss: 7.943362760543822 Accumlated Loss: 12.182324503213167\n",
    "[Step 20/100] Immediate Loss: 7.575157041847708 Accumlated Loss: 7.837963561832905\n",
    "[Step 30/100] Immediate Loss: 6.971591109633447 Accumlated Loss: 7.41288396987319\n",
    "[Step 40/100] Immediate Loss: 7.35790877491236 Accumlated Loss: 7.329814511984587\n",
    "[Step 50/100] Immediate Loss: 7.227651714682577 Accumlated Loss: 7.152353108674289\n",
    "[Step 60/100] Immediate Loss: 7.120855849683284 Accumlated Loss: 7.094745240241287\n",
    "[Step 70/100] Immediate Loss: 6.898626097440719 Accumlated Loss: 6.994988939702512\n",
    "[Step 80/100] Immediate Loss: 6.973135902881624 Accumlated Loss: 6.9992388206422325\n",
    "[Step 90/100] Immediate Loss: 6.978269910216333 Accumlated Loss: 6.970272527605296\n",
    "[Step 100/100] Immediate Loss: 6.8891827100515375 Accumlated Loss: 6.946669360905885"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "major-proceeding",
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_made\n",
    "hidden_size = 256\n",
    "[Step 10/300] Immediate Loss: 7.3377627801895144 Accumlated Loss: 10.410332291990517\n",
    "[Step 20/300] Immediate Loss: 7.159581978321075 Accumlated Loss: 7.200047956764698\n",
    "[Step 30/300] Immediate Loss: 7.061564602553847 Accumlated Loss: 7.006011275857687\n",
    "[Step 40/300] Immediate Loss: 6.9438557976484265 Accumlated Loss: 6.986207850277424\n",
    "[Step 50/300] Immediate Loss: 6.79013857513666 Accumlated Loss: 6.907507795482872\n",
    "[Step 60/300] Immediate Loss: 6.964799135029316 Accumlated Loss: 6.948932709783316\n",
    "[Step 70/300] Immediate Loss: 6.827503361701965 Accumlated Loss: 6.935307196795939\n",
    "[Step 80/300] Immediate Loss: 6.932054169476035 Accumlated Loss: 6.893120049089195\n",
    "[Step 90/300] Immediate Loss: 6.959000964760777 Accumlated Loss: 6.910556671291589\n",
    "[Step 100/300] Immediate Loss: 6.850004935860634 Accumlated Loss: 6.864318913996221\n",
    "[Step 110/300] Immediate Loss: 6.94344320565462 Accumlated Loss: 6.850439666450024\n",
    "[Step 120/300] Immediate Loss: 6.898613585829735 Accumlated Loss: 6.8139530434310425\n",
    "[Step 130/300] Immediate Loss: 6.832337006330492 Accumlated Loss: 6.831135106772184\n",
    "[Step 140/300] Immediate Loss: 6.981345642507076 Accumlated Loss: 6.840172601640226\n",
    "[Step 150/300] Immediate Loss: 6.8799775546789155 Accumlated Loss: 6.84133131918311\n",
    "[Step 160/300] Immediate Loss: 6.955301420390605 Accumlated Loss: 6.884279209256171\n",
    "[Step 170/300] Immediate Loss: 6.737140662968159 Accumlated Loss: 6.837186959028243\n",
    "[Step 180/300] Immediate Loss: 6.770348101556301 Accumlated Loss: 6.8123826805353165\n",
    "[Step 190/300] Immediate Loss: 6.84570457994938 Accumlated Loss: 6.796486026227475\n",
    "[Step 200/300] Immediate Loss: 6.918634173870087 Accumlated Loss: 6.7889503496289265\n",
    "[Step 210/300] Immediate Loss: 6.818292626738547 Accumlated Loss: 6.842672590941191\n",
    "[Step 220/300] Immediate Loss: 6.885442342460156 Accumlated Loss: 6.7800200353860856\n",
    "[Step 230/300] Immediate Loss: 6.8983984541893 Accumlated Loss: 6.816990964740515\n",
    "[Step 240/300] Immediate Loss: 6.703417011201383 Accumlated Loss: 6.77779578369856\n",
    "[Step 250/300] Immediate Loss: 6.782429467439654 Accumlated Loss: 6.791635487049819\n",
    "[Step 260/300] Immediate Loss: 6.779994890689852 Accumlated Loss: 6.800842138975859\n",
    "[Step 270/300] Immediate Loss: 6.651651687920095 Accumlated Loss: 6.76608702453971\n",
    "[Step 280/300] Immediate Loss: 6.800747621953487 Accumlated Loss: 6.8188140452504165\n",
    "[Step 290/300] Immediate Loss: 6.772528566718105 Accumlated Loss: 6.796392483770848\n",
    "[Step 300/300] Immediate Loss: 6.761312774717806 Accumlated Loss: 6.7664973594844335"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "level-return",
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_made\n",
    "hidden_size = 512\n",
    "[Step 10/200] Immediate Loss: 7.271965174973012 Accumlated Loss: 9.238334188848734\n",
    "[Step 20/200] Immediate Loss: 7.186947046220303 Accumlated Loss: 7.23573069420457\n",
    "[Step 30/200] Immediate Loss: 7.122338608205319 Accumlated Loss: 7.092619772553443\n",
    "[Step 40/200] Immediate Loss: 6.837136044204232 Accumlated Loss: 7.121531615853309\n",
    "[Step 50/200] Immediate Loss: 6.922652315497399 Accumlated Loss: 7.035050431400538\n",
    "[Step 60/200] Immediate Loss: 7.0224101865291635 Accumlated Loss: 7.050180885791779\n",
    "[Step 70/200] Immediate Loss: 6.878841565251351 Accumlated Loss: 6.977283853858709\n",
    "[Step 80/200] Immediate Loss: 6.917600260972978 Accumlated Loss: 6.960183880478144\n",
    "[Step 90/200] Immediate Loss: 7.015150077044962 Accumlated Loss: 6.9326501165926455\n",
    "[Step 100/200] Immediate Loss: 6.942776599228382 Accumlated Loss: 6.940360210925341\n",
    "[Step 110/200] Immediate Loss: 6.844236496090893 Accumlated Loss: 6.884334388613701\n",
    "[Step 120/200] Immediate Loss: 6.8093515345454225 Accumlated Loss: 6.9124150166511535\n",
    "[Step 130/200] Immediate Loss: 6.829742189943792 Accumlated Loss: 6.877818616390228\n",
    "[Step 140/200] Immediate Loss: 6.9080816498398745 Accumlated Loss: 6.879785261124372\n",
    "[Step 150/200] Immediate Loss: 6.924012284278872 Accumlated Loss: 6.912908691138029\n",
    "[Step 160/200] Immediate Loss: 6.945983737111093 Accumlated Loss: 6.874559896111489\n",
    "[Step 170/200] Immediate Loss: 6.915178183317185 Accumlated Loss: 6.880107059150934\n",
    "[Step 180/200] Immediate Loss: 6.651397094726563 Accumlated Loss: 6.78183588451147\n",
    "[Step 190/200] Immediate Loss: 6.8623131331801455 Accumlated Loss: 6.819020005971194\n",
    "[Step 200/200] Immediate Loss: 6.910607502162455 Accumlated Loss: 6.900225434094667"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "artistic-wagner",
   "metadata": {},
   "outputs": [],
   "source": [
    "### guide 5_1_1 Reference\n",
    "[Step 10/200] Immediate Loss: 7.62717 Accumlated Loss: 10.73827 Duration: 7.676\n",
    "[Step 20/200] Immediate Loss: 7.26588 Accumlated Loss: 7.48846 Duration: 7.717\n",
    "[Step 30/200] Immediate Loss: 6.91100 Accumlated Loss: 7.07743 Duration: 7.722\n",
    "[Step 40/200] Immediate Loss: 6.83163 Accumlated Loss: 6.94489 Duration: 7.689\n",
    "[Step 50/200] Immediate Loss: 6.89025 Accumlated Loss: 6.88702 Duration: 7.662\n",
    "[Step 60/200] Immediate Loss: 6.86455 Accumlated Loss: 6.92596 Duration: 7.748\n",
    "[Step 70/200] Immediate Loss: 6.72298 Accumlated Loss: 6.82665 Duration: 7.640\n",
    "[Step 80/200] Immediate Loss: 6.85124 Accumlated Loss: 6.84992 Duration: 7.598\n",
    "[Step 90/200] Immediate Loss: 6.85684 Accumlated Loss: 6.83059 Duration: 7.608\n",
    "[Step 100/200] Immediate Loss: 6.70233 Accumlated Loss: 6.82399 Duration: 7.613\n",
    "[Step 110/200] Immediate Loss: 6.72158 Accumlated Loss: 6.79653 Duration: 7.902\n",
    "[Step 120/200] Immediate Loss: 6.86706 Accumlated Loss: 6.81691 Duration: 7.493\n",
    "[Step 130/200] Immediate Loss: 6.73154 Accumlated Loss: 6.78102 Duration: 7.514\n",
    "[Step 140/200] Immediate Loss: 6.74963 Accumlated Loss: 6.79802 Duration: 7.489\n",
    "[Step 150/200] Immediate Loss: 6.77680 Accumlated Loss: 6.80260 Duration: 7.672\n",
    "[Step 160/200] Immediate Loss: 6.81672 Accumlated Loss: 6.80855 Duration: 7.539\n",
    "[Step 170/200] Immediate Loss: 6.81623 Accumlated Loss: 6.77428 Duration: 7.604\n",
    "[Step 180/200] Immediate Loss: 6.72911 Accumlated Loss: 6.75416 Duration: 7.737\n",
    "[Step 190/200] Immediate Loss: 6.65551 Accumlated Loss: 6.74630 Duration: 7.945\n",
    "[Step 200/200] Immediate Loss: 6.78092 Accumlated Loss: 6.77993 Duration: 7.496"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "contemporary-bahamas",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "still-impression",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
