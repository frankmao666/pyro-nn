{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "optional-directory",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor(5.6887), tensor(3.8709), tensor(8.2887), tensor(3.4577), tensor(3.2402), tensor(4.2094), tensor(9.3640), tensor(4.5493), tensor(3.6753), tensor(7.4958), tensor(4.4362), tensor(3.6653), tensor(5.0407), tensor(4.1607), tensor(4.0133), tensor(5.4690), tensor(3.4579), tensor(4.3558), tensor(12.5317), tensor(7.3944), tensor(4.9683), tensor(4.1077), tensor(-2.8826), tensor(2.4679), tensor(8.7158), tensor(5.5844), tensor(6.3157), tensor(4.9109), tensor(4.0527), tensor(4.9249), tensor(4.1243), tensor(3.1873), tensor(3.6325), tensor(5.7898), tensor(4.3374), tensor(4.4134), tensor(3.9700), tensor(5.0561), tensor(5.4618), tensor(3.7995), tensor(2.6572), tensor(5.2714), tensor(1.6367), tensor(2.2350), tensor(0.3866), tensor(8.7246), tensor(3.6449), tensor(3.8202), tensor(4.4491), tensor(7.3479), tensor(5.2542), tensor(4.3766), tensor(2.2654), tensor(-0.6813), tensor(4.3614), tensor(5.2052), tensor(1.2884), tensor(5.3898), tensor(4.6965), tensor(3.0395), tensor(4.7798), tensor(4.6280), tensor(-0.1253), tensor(7.1224), tensor(5.1034), tensor(4.6985), tensor(1.1765), tensor(4.5949), tensor(4.3945), tensor(4.5083), tensor(4.9237), tensor(0.4142), tensor(2.6376), tensor(4.5633), tensor(0.3433), tensor(2.8833), tensor(5.8162), tensor(4.4944), tensor(6.4083), tensor(1.0054), tensor(3.0098), tensor(4.4467), tensor(4.6551), tensor(4.7661), tensor(4.9026), tensor(3.4916), tensor(3.7257), tensor(6.2789), tensor(4.5558), tensor(0.9115), tensor(3.5769), tensor(4.9897), tensor(8.3029), tensor(4.3912), tensor(4.1931), tensor(3.9703), tensor(5.1790), tensor(2.3602), tensor(-0.4865), tensor(2.6088), tensor(6.0368), tensor(5.3541), tensor(6.8754), tensor(6.1961), tensor(5.0526), tensor(4.4814), tensor(5.3695), tensor(2.6375), tensor(5.3977), tensor(5.3021), tensor(3.9568), tensor(4.3696), tensor(3.0917), tensor(4.3970), tensor(6.3894), tensor(8.3249), tensor(3.9408), tensor(1.6646), tensor(3.7495), tensor(4.4161), tensor(4.3915), tensor(0.1780), tensor(6.2597), tensor(2.3313), tensor(4.1748), tensor(4.4064), tensor(4.8123), tensor(8.2522), tensor(4.2261), tensor(5.6417), tensor(2.4327), tensor(2.8164), tensor(5.1308), tensor(2.8427), tensor(-0.3952), tensor(10.0646), tensor(3.7087), tensor(8.3217), tensor(3.6316), tensor(3.8514), tensor(-1.9646), tensor(4.1923), tensor(1.1500), tensor(4.5213), tensor(4.2822), tensor(3.8307), tensor(1.4602), tensor(4.3398), tensor(-0.3874), tensor(3.1578), tensor(5.6799), tensor(4.6157), tensor(4.8051), tensor(7.0650), tensor(1.2613), tensor(3.8232), tensor(6.9480), tensor(-1.1388), tensor(3.0873), tensor(3.5469), tensor(5.4195), tensor(4.7246), tensor(-0.1574), tensor(4.5460), tensor(1.6255), tensor(1.6854), tensor(4.3571), tensor(3.7877), tensor(3.6491), tensor(4.6810), tensor(2.5066), tensor(-0.0739), tensor(2.9757), tensor(5.0429), tensor(4.6754), tensor(3.8501), tensor(4.0477), tensor(4.0458), tensor(4.9095), tensor(4.3014), tensor(-2.8129), tensor(2.9261), tensor(0.5175), tensor(4.4074), tensor(3.9923), tensor(3.2315), tensor(5.0637), tensor(5.7258), tensor(4.4181), tensor(4.8514), tensor(1.5365), tensor(3.0838), tensor(5.4712), tensor(5.2525), tensor(7.1797), tensor(2.0392), tensor(3.9848), tensor(3.4315), tensor(10.8305), tensor(9.1264)]\n",
      "[Step 10/200] Immediate Loss: 7.195491719357669 Accumlated Loss: 39.700878605598575\n",
      "[Step 20/200] Immediate Loss: 5.6998961744457475 Accumlated Loss: 6.26671837088093\n",
      "[Step 30/200] Immediate Loss: 4.692294080862777 Accumlated Loss: 4.946705584792887\n",
      "[Step 40/200] Immediate Loss: 4.796591249112097 Accumlated Loss: 4.752417349870171\n",
      "[Step 50/200] Immediate Loss: 4.621495101320762 Accumlated Loss: 4.670041528392663\n",
      "[Step 60/200] Immediate Loss: 4.534939376313593 Accumlated Loss: 4.618246412111732\n",
      "[Step 70/200] Immediate Loss: 4.585024912086808 Accumlated Loss: 4.551269153323987\n",
      "[Step 80/200] Immediate Loss: 4.454118347572948 Accumlated Loss: 4.474291823778582\n",
      "[Step 90/200] Immediate Loss: 4.4294447009708335 Accumlated Loss: 4.472697340578821\n",
      "[Step 100/200] Immediate Loss: 4.429915012706409 Accumlated Loss: 4.477198233705097\n",
      "[Step 110/200] Immediate Loss: 4.5613434716909556 Accumlated Loss: 4.455384294834135\n",
      "[Step 120/200] Immediate Loss: 4.443874849736525 Accumlated Loss: 4.425778731042524\n",
      "[Step 130/200] Immediate Loss: 4.571623991941054 Accumlated Loss: 4.450863315437208\n",
      "[Step 140/200] Immediate Loss: 4.510091479033561 Accumlated Loss: 4.427214763087768\n",
      "[Step 150/200] Immediate Loss: 4.523874952274341 Accumlated Loss: 4.427721300816889\n",
      "[Step 160/200] Immediate Loss: 4.454986119801448 Accumlated Loss: 4.432565965245174\n",
      "[Step 170/200] Immediate Loss: 4.372630771429708 Accumlated Loss: 4.390093631051786\n",
      "[Step 180/200] Immediate Loss: 4.448110207207775 Accumlated Loss: 4.423109177469071\n",
      "[Step 190/200] Immediate Loss: 4.415633551918446 Accumlated Loss: 4.4107278995792125\n",
      "[Step 200/200] Immediate Loss: 4.496516553735528 Accumlated Loss: 4.427401870993286\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pyro\n",
    "import pyro.contrib.examples.polyphonic_data_loader as poly\n",
    "import pyro.distributions as dist\n",
    "from pyro.infer import SVI, Trace_ELBO\n",
    "from pyro.optim import Adam\n",
    "from matplotlib import pyplot as plt\n",
    "from made import MADE\n",
    "\n",
    "random.seed(234) # 123\n",
    "torch.manual_seed(234)\n",
    "\n",
    "# NN used for p(x | y)\n",
    "class simpleNN(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden=32, out_size=1, t=\"normal\", out_non_linear=None):\n",
    "        super().__init__()\n",
    "        self.t = t\n",
    "        self.out_non_linear = out_non_linear\n",
    "        self.hiddeen_layer = nn.Linear(input_size, hidden)\n",
    "        if t == \"normal\":\n",
    "            self.loc_layer = nn.Linear(hidden, out_size)\n",
    "            self.std_layer = nn.Linear(hidden, out_size)\n",
    "            self.softplus = nn.Softplus()\n",
    "        elif t == \"bern\":\n",
    "            self.prob_layer = nn.Linear(hidden, out_size)\n",
    "        elif t == \"mlp\":\n",
    "            self.out_layer = nn.Linear(hidden, out_size)\n",
    "        \n",
    "    def forward(self, x_list):\n",
    "        for i in range(len(x_list)):\n",
    "            if x_list[i].dim() == 0:\n",
    "                x_list[i] = torch.unsqueeze(x_list[i], dim=0)\n",
    "        input_x = torch.cat(x_list)\n",
    "        hid = F.relu(self.hiddeen_layer(input_x))\n",
    "        # return loc, std\n",
    "        if self.t == \"normal\":\n",
    "            return self.loc_layer(hid), self.softplus(self.std_layer(hid))\n",
    "        elif self.t == \"bern\":\n",
    "            return torch.sigmoid(self.prob_layer(hid))\n",
    "        else:\n",
    "            if self.out_non_linear == \"tanh\":\n",
    "                return torch.tanh(self.out_layer(hid))\n",
    "            else:\n",
    "                return self.out_layer(hid)\n",
    "        \n",
    "class simpleRNN(nn.Module):\n",
    "    def __init__(self, input_size=2, hidden_size=32, max_l=5, max_t=3):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.RNN(input_size=input_size + max_l + max_t, hidden_size=hidden_size, nonlinearity='relu',\n",
    "                          batch_first=True, num_layers=1)\n",
    "        self.h_0 = nn.Parameter(torch.zeros((1, 1, hidden_size)))\n",
    "        self.out_loc = nn.Linear(hidden_size, 1)\n",
    "        self.out_std = nn.Linear(hidden_size, 1)\n",
    "        self.max_l = max_l # max time steps\n",
    "        self.max_t = max_t # type of random variables, ex. y1 y2 next_x\n",
    "        self.softplus = nn.Softplus()\n",
    "        \n",
    "    def forward(self, x, obs, l, t):\n",
    "        \"\"\"\n",
    "        x: x0\n",
    "        obs: R\n",
    "        l: length\n",
    "        t: type, y1, y2 or next_x\n",
    "        \"\"\"\n",
    "        length = l * 3\n",
    "        input_x = x.repeat((int(length), 1))\n",
    "        input_obs = obs.repeat((int(length), 1))\n",
    "        input_l = []\n",
    "        input_t = []\n",
    "        \n",
    "        for n in range(int(l)):\n",
    "            \n",
    "            for i in range(int(t)):\n",
    "                input_l.append(n)\n",
    "                input_t.append(i)\n",
    "        input_l = F.one_hot(torch.tensor(input_l), self.max_l) \n",
    "        input_t = F.one_hot(torch.tensor(input_t), self.max_t) \n",
    "        \n",
    "        input_ = torch.unsqueeze(torch.cat([input_x, input_obs, input_l, input_t], -1), 0)\n",
    "        \n",
    "        # the input is [x, obs, onehot(l), onehot(t)]\n",
    "        rnn_output, _ = self.rnn(input_, self.h_0)\n",
    "        rnn_output = torch.squeeze(rnn_output, 0)\n",
    "        out_loc = self.out_loc(F.relu(rnn_output))\n",
    "        out_std = self.softplus(self.out_std(F.relu(rnn_output)))\n",
    "        # the first outputs are y_1_1, y_2_1, next_x_1; y_1_2, y_2_2, next_x_2\n",
    "        return torch.squeeze(out_loc, 1), torch.squeeze(out_std, 1) # shape l * t\n",
    "\n",
    "class Experiment(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden_size = 3\n",
    "        self.max_rec = 5\n",
    "        \n",
    "        # for guide_0\n",
    "        self.x_nn_0 = simpleNN(input_size=1, t=\"bern\")\n",
    "        self.y_nn_1_0 = simpleNN(input_size=1)\n",
    "        self.y_nn_2_0 = simpleNN(input_size=1)\n",
    "    \n",
    "        # for guide_1\n",
    "        self.x_nn_1 = simpleNN(input_size=1, t=\"bern\")\n",
    "        self.y_nn_1_1 = simpleNN(input_size=2)\n",
    "        self.y_nn_2_1 = simpleNN(input_size=2)\n",
    "    \n",
    "    def g(self, x):\n",
    "        return torch.tanh(x) \n",
    "    \n",
    "    def model(self, obs):\n",
    "        pyro.module(\"model\", self)\n",
    "        \n",
    "        x = pyro.sample(\"x\", dist.Bernoulli(0.5))\n",
    "        \n",
    "        if x > 0:\n",
    "            y =  pyro.sample(\"y\", dist.Normal(1.5, 1))\n",
    "        else:\n",
    "            y =  pyro.sample(\"y\", dist.Normal(3, 1))\n",
    "        \n",
    "        sig = torch.tensor(0.1)\n",
    "        \n",
    "        pyro.sample(\"obs\", dist.Normal(y, sig), obs=float(obs))\n",
    "        \n",
    "\n",
    "    # guide uses simple and individual NN for each random variable\n",
    "    def guide_0(self, obs):\n",
    "        pyro.module(\"model\", self)\n",
    "       \n",
    "        x_prob = self.x_nn_0([obs])\n",
    "        x = pyro.sample(\"x\", dist.Bernoulli(x_prob))\n",
    "        \n",
    "        if x > 0:\n",
    "            y_loc, y_std = self.y_nn_1_0([obs])\n",
    "            y =  pyro.sample(\"y\", dist.Normal(y_loc, y_std))\n",
    "        else:\n",
    "            y_loc, y_std = self.y_nn_2_0([obs])\n",
    "            y =  pyro.sample(\"y\", dist.Normal(y_loc, y_std))\n",
    "    \n",
    "    # guide uses simple and individual NN for each random variable\n",
    "    def guide_1(self, obs):\n",
    "        pyro.module(\"model\", self)\n",
    "       \n",
    "        x_prob = self.x_nn_1([obs])\n",
    "        x = pyro.sample(\"x\", dist.Bernoulli(x_prob))\n",
    "        \n",
    "        if x > 0:\n",
    "            #y_loc, y_std = self.y_nn_1_1([obs, x_prob])\n",
    "            y_loc, y_std = self.y_nn_1_1([obs, x])\n",
    "            y =  pyro.sample(\"y\", dist.Normal(y_loc, y_std))\n",
    "        else:\n",
    "            #y_loc, y_std = self.y_nn_2_1([obs, x_prob])\n",
    "            y_loc, y_std = self.y_nn_2_1([obs, x])\n",
    "            y =  pyro.sample(\"y\", dist.Normal(y_loc, y_std))\n",
    "            \n",
    "    \n",
    "def generate_data():\n",
    "    # the actual data generation has three latent variables (y_1, y_2, y_3)\n",
    "    x_prob = max(min(random.random() + dist.Normal(0.2, 0.5).sample(), 0.9), 0.1)\n",
    "    \n",
    "    x = dist.Bernoulli(x_prob).sample()\n",
    "    \n",
    "    if x > 0:\n",
    "        mean = random.random() * 3 + 2\n",
    "        std = random.random() + 0.658\n",
    "        y = dist.Normal(mean, std).sample()\n",
    "        y = dist.Normal(y + random.random(), std*2).sample()\n",
    "    else:\n",
    "        mean = random.random() * 1 + 4\n",
    "        std = random.random() + 0.152\n",
    "        y = dist.Normal(mean, std).sample()\n",
    "    return y\n",
    "    \n",
    "data = []\n",
    "num_data = 200 # 100\n",
    "for _ in range(num_data):\n",
    "    data.append(generate_data())\n",
    "\n",
    "print(data)\n",
    "experiment = Experiment()\n",
    "adam_params = {\"lr\": 0.001, \"betas\": (0.95, 0.999)}\n",
    "optimizer = Adam(adam_params)\n",
    "guide = experiment.guide_0 # guide_1\n",
    "\n",
    "svi = SVI(experiment.model, guide, optimizer, loss=Trace_ELBO())\n",
    "\n",
    "n_steps = 200\n",
    "log_interval = 10\n",
    "# do gradient steps\n",
    "loss = 0\n",
    "loss_track = []\n",
    "for step in range(1, n_steps + 1):\n",
    "    imme_loss = 0\n",
    "    \n",
    "    for obs in data:\n",
    "        imme_loss += svi.step(obs) / num_data\n",
    "        \n",
    "    loss_track.append(imme_loss)\n",
    "    loss += imme_loss / log_interval\n",
    "    \n",
    "    if step % log_interval == 0:\n",
    "        print(\"[Step {}/{}] Immediate Loss: {} Accumlated Loss: {}\".format(step, n_steps, imme_loss, loss))\n",
    "        loss = 0\n",
    "    \n",
    "plt.plot(loss_track)\n",
    "plt.title(\"Loss\")\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "systematic-paradise",
   "metadata": {},
   "source": [
    "results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "based-reader",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-2-2b56b9eb846d>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-2-2b56b9eb846d>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    [Step 10/200] Immediate Loss: 8.481406583273783 Accumlated Loss: 138.92595821277888\u001b[0m\n\u001b[1;37m          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# guide_0\n",
    "[Step 10/200] Immediate Loss: 8.481406583273783 Accumlated Loss: 138.92595821277888\n",
    "[Step 20/200] Immediate Loss: 5.827122594807298 Accumlated Loss: 6.824616750597954\n",
    "[Step 30/200] Immediate Loss: 5.479589960807936 Accumlated Loss: 6.122400292096194\n",
    "[Step 40/200] Immediate Loss: 5.235114186722786 Accumlated Loss: 5.868071163090412\n",
    "[Step 50/200] Immediate Loss: 5.230031159862411 Accumlated Loss: 5.398840152133023\n",
    "[Step 60/200] Immediate Loss: 4.941381576012355 Accumlated Loss: 5.2451702640415165\n",
    "[Step 70/200] Immediate Loss: 5.1643944143434055 Accumlated Loss: 5.1688680721505085\n",
    "[Step 80/200] Immediate Loss: 4.977463321115354 Accumlated Loss: 5.088783462397755\n",
    "[Step 90/200] Immediate Loss: 4.915429681032547 Accumlated Loss: 4.998852465530391\n",
    "[Step 100/200] Immediate Loss: 4.889856382282449 Accumlated Loss: 4.930096195126055\n",
    "[Step 110/200] Immediate Loss: 5.036540711353885 Accumlated Loss: 4.943592827759556\n",
    "[Step 120/200] Immediate Loss: 5.039725264719089 Accumlated Loss: 4.930605179857425\n",
    "[Step 130/200] Immediate Loss: 5.059065213970024 Accumlated Loss: 4.90860458523815\n",
    "[Step 140/200] Immediate Loss: 4.940604259062165 Accumlated Loss: 4.980269359458469\n",
    "[Step 150/200] Immediate Loss: 4.902322931521559 Accumlated Loss: 4.932228341744303\n",
    "[Step 160/200] Immediate Loss: 5.1983008842096385 Accumlated Loss: 4.950344393348063\n",
    "[Step 170/200] Immediate Loss: 4.806001321339315 Accumlated Loss: 4.889006002039204\n",
    "[Step 180/200] Immediate Loss: 4.986952164080137 Accumlated Loss: 4.875345367057301\n",
    "[Step 190/200] Immediate Loss: 4.8897376226142955 Accumlated Loss: 4.904684411775412\n",
    "[Step 200/200] Immediate Loss: 5.004684399832466 Accumlated Loss: 4.949257210554612"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "close-context",
   "metadata": {},
   "outputs": [],
   "source": [
    "# guide_1\n",
    "[Step 10/200] Immediate Loss: 11.723265527635808 Accumlated Loss: 158.07744119115924\n",
    "[Step 20/200] Immediate Loss: 7.0115033677220335 Accumlated Loss: 7.743059941321611\n",
    "[Step 30/200] Immediate Loss: 5.568605363368985 Accumlated Loss: 6.196570287048817\n",
    "[Step 40/200] Immediate Loss: 5.199520883075892 Accumlated Loss: 5.525774829182774\n",
    "[Step 50/200] Immediate Loss: 5.025597251914441 Accumlated Loss: 5.208455127271823\n",
    "[Step 60/200] Immediate Loss: 4.85863864256069 Accumlated Loss: 5.040611718276979\n",
    "[Step 70/200] Immediate Loss: 5.104041776752566 Accumlated Loss: 4.983732196276776\n",
    "[Step 80/200] Immediate Loss: 4.867030485807919 Accumlated Loss: 4.9212386325227575\n",
    "[Step 90/200] Immediate Loss: 4.8761734484438755 Accumlated Loss: 4.913726011077641\n",
    "[Step 100/200] Immediate Loss: 5.023541996709683 Accumlated Loss: 4.91273052565677\n",
    "[Step 110/200] Immediate Loss: 4.940182206740063 Accumlated Loss: 4.902736147870894\n",
    "[Step 120/200] Immediate Loss: 4.8979976212938245 Accumlated Loss: 4.907227212036015\n",
    "[Step 130/200] Immediate Loss: 4.938307256418921 Accumlated Loss: 4.86265199468225\n",
    "[Step 140/200] Immediate Loss: 4.856055798890488 Accumlated Loss: 4.916034141579493\n",
    "[Step 150/200] Immediate Loss: 4.878045419446584 Accumlated Loss: 4.929280334066892\n",
    "[Step 160/200] Immediate Loss: 5.044146585931762 Accumlated Loss: 4.917067585667423\n",
    "[Step 170/200] Immediate Loss: 4.772590825204636 Accumlated Loss: 4.860669169802714\n",
    "[Step 180/200] Immediate Loss: 4.928167786201757 Accumlated Loss: 4.872080631799747\n",
    "[Step 190/200] Immediate Loss: 4.857129885650064 Accumlated Loss: 4.890450273939073\n",
    "[Step 200/200] Immediate Loss: 4.896220705355964 Accumlated Loss: 4.882933780943065"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "muslim-heritage",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "urban-tension",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "destroyed-massage",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nonprofit-providence",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0\n",
    "[Step 10/200] Immediate Loss: 6.329348330907526 Accumlated Loss: 83.74248697780818\n",
    "[Step 20/200] Immediate Loss: 4.78288633824326 Accumlated Loss: 5.276437185511925\n",
    "[Step 30/200] Immediate Loss: 4.7403195415437205 Accumlated Loss: 4.750574695361779\n",
    "[Step 40/200] Immediate Loss: 4.638174880848966 Accumlated Loss: 4.5884155824468476\n",
    "[Step 50/200] Immediate Loss: 4.653941555392811 Accumlated Loss: 4.513697536185791\n",
    "[Step 60/200] Immediate Loss: 4.627054877117982 Accumlated Loss: 4.530043110702847\n",
    "[Step 70/200] Immediate Loss: 4.464643498879968 Accumlated Loss: 4.474516210590077\n",
    "[Step 80/200] Immediate Loss: 4.439730934067952 Accumlated Loss: 4.488008958363516\n",
    "[Step 90/200] Immediate Loss: 4.53654706622618 Accumlated Loss: 4.456650750458131\n",
    "[Step 100/200] Immediate Loss: 4.400493721984981 Accumlated Loss: 4.430780795232551\n",
    "[Step 110/200] Immediate Loss: 4.509002554508242 Accumlated Loss: 4.475029762922149\n",
    "[Step 120/200] Immediate Loss: 4.413638624070391 Accumlated Loss: 4.438440319239027\n",
    "[Step 130/200] Immediate Loss: 4.5498218181579 Accumlated Loss: 4.471651754439898\n",
    "[Step 140/200] Immediate Loss: 4.373417100855421 Accumlated Loss: 4.428907605508969\n",
    "[Step 150/200] Immediate Loss: 4.549445116027336 Accumlated Loss: 4.432188125092466\n",
    "[Step 160/200] Immediate Loss: 4.544727864170218 Accumlated Loss: 4.443647913407853\n",
    "[Step 170/200] Immediate Loss: 4.4030221931870255 Accumlated Loss: 4.408120165453587\n",
    "[Step 180/200] Immediate Loss: 4.396375845459053 Accumlated Loss: 4.432364422364805\n",
    "[Step 190/200] Immediate Loss: 4.432063233334848 Accumlated Loss: 4.40650718549003\n",
    "[Step 200/200] Immediate Loss: 4.410849580574181 Accumlated Loss: 4.397266691096533"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loose-driver",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yellow-andrew",
   "metadata": {},
   "outputs": [],
   "source": [
    "0: 64 dim\n",
    "[Step 10/200] Immediate Loss: 4.892548137257108 Accumlated Loss: 25.175123196755194\n",
    "[Step 20/200] Immediate Loss: 4.758278623902004 Accumlated Loss: 4.710510768516105\n",
    "[Step 30/200] Immediate Loss: 4.588608430172572 Accumlated Loss: 4.763768781696699\n",
    "[Step 40/200] Immediate Loss: 4.675227941598608 Accumlated Loss: 4.660967427416472\n",
    "[Step 50/200] Immediate Loss: 4.5980175924641795 Accumlated Loss: 4.690014428250329\n",
    "[Step 60/200] Immediate Loss: 4.452137063505405 Accumlated Loss: 4.668534330072428\n",
    "[Step 70/200] Immediate Loss: 4.677649372447063 Accumlated Loss: 4.598335882357667\n",
    "[Step 80/200] Immediate Loss: 4.690546879154372 Accumlated Loss: 4.568423189383524\n",
    "[Step 90/200] Immediate Loss: 4.502625433626427 Accumlated Loss: 4.535526312640294\n",
    "[Step 100/200] Immediate Loss: 4.507959322865512 Accumlated Loss: 4.539393051659962\n",
    "[Step 110/200] Immediate Loss: 4.577937096728701 Accumlated Loss: 4.536217603941878\n",
    "[Step 120/200] Immediate Loss: 4.557467424991199 Accumlated Loss: 4.539361650452866\n",
    "[Step 130/200] Immediate Loss: 4.543855754571498 Accumlated Loss: 4.523867381748434\n",
    "[Step 140/200] Immediate Loss: 4.601759381196598 Accumlated Loss: 4.540427309621547\n",
    "[Step 150/200] Immediate Loss: 4.455749744192999 Accumlated Loss: 4.497635044049465\n",
    "[Step 160/200] Immediate Loss: 4.559713857203792 Accumlated Loss: 4.515638842284788\n",
    "[Step 170/200] Immediate Loss: 4.506635000804083 Accumlated Loss: 4.523887936461107\n",
    "[Step 180/200] Immediate Loss: 4.531122605114395 Accumlated Loss: 4.502436893166829\n",
    "[Step 190/200] Immediate Loss: 4.4107590396855745 Accumlated Loss: 4.458605431016796\n",
    "[Step 200/200] Immediate Loss: 4.406933291976797 Accumlated Loss: 4.473983885180661"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loose-perception",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 \n",
    "[Step 10/200] Immediate Loss: 8.99335336676194 Accumlated Loss: 93.11557751736001\n",
    "[Step 20/200] Immediate Loss: 7.757730990499698 Accumlated Loss: 8.150530896322358\n",
    "[Step 30/200] Immediate Loss: 7.251889504697169 Accumlated Loss: 7.62071710877218\n",
    "[Step 40/200] Immediate Loss: 4.677049168914674 Accumlated Loss: 4.972788094645658\n",
    "[Step 50/200] Immediate Loss: 4.739154729392898 Accumlated Loss: 4.543627997050383\n",
    "[Step 60/200] Immediate Loss: 4.645770498027884 Accumlated Loss: 4.506981913097405\n",
    "[Step 70/200] Immediate Loss: 4.431528997150307 Accumlated Loss: 4.464423873282858\n",
    "[Step 80/200] Immediate Loss: 4.463611593075621 Accumlated Loss: 4.45461358745756\n",
    "[Step 90/200] Immediate Loss: 4.604924235458547 Accumlated Loss: 4.454517559632595\n",
    "[Step 100/200] Immediate Loss: 4.401257157894304 Accumlated Loss: 4.4150539258585635\n",
    "[Step 110/200] Immediate Loss: 4.469707039907149 Accumlated Loss: 4.439741982026343\n",
    "[Step 120/200] Immediate Loss: 4.400099222563222 Accumlated Loss: 4.420476458520239\n",
    "[Step 130/200] Immediate Loss: 4.4864507112593275 Accumlated Loss: 4.46372244460293\n",
    "[Step 140/200] Immediate Loss: 4.3791043448489475 Accumlated Loss: 4.41920482103407\n",
    "[Step 150/200] Immediate Loss: 4.513532081914066 Accumlated Loss: 4.425186696831541\n",
    "[Step 160/200] Immediate Loss: 4.453900899845154 Accumlated Loss: 4.43659522759344\n",
    "[Step 170/200] Immediate Loss: 4.3917057774518975 Accumlated Loss: 4.397434466401062\n",
    "[Step 180/200] Immediate Loss: 4.405244329166033 Accumlated Loss: 4.426350365344331\n",
    "[Step 190/200] Immediate Loss: 4.429013590349041 Accumlated Loss: 4.410054011311578\n",
    "[Step 200/200] Immediate Loss: 4.415098900823153 Accumlated Loss: 4.394547299068895"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "packed-springer",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proud-authentication",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 64\n",
    "[Step 10/200] Immediate Loss: 4.945362403555771 Accumlated Loss: 27.69191990887678\n",
    "[Step 20/200] Immediate Loss: 4.705575195045094 Accumlated Loss: 4.74906130639941\n",
    "[Step 30/200] Immediate Loss: 4.698617646599962 Accumlated Loss: 4.640836778211009\n",
    "[Step 40/200] Immediate Loss: 4.597827718715852 Accumlated Loss: 4.6130812472309035\n",
    "[Step 50/200] Immediate Loss: 4.473430356117529 Accumlated Loss: 4.592548839382395\n",
    "[Step 60/200] Immediate Loss: 4.46108817300642 Accumlated Loss: 4.592729462655066\n",
    "[Step 70/200] Immediate Loss: 4.5765040191570705 Accumlated Loss: 4.541705991156426\n",
    "[Step 80/200] Immediate Loss: 4.615994801401456 Accumlated Loss: 4.527830253832891\n",
    "[Step 90/200] Immediate Loss: 4.481257107800941 Accumlated Loss: 4.490467119288146\n",
    "[Step 100/200] Immediate Loss: 4.447113471263 Accumlated Loss: 4.491194007766871\n",
    "[Step 110/200] Immediate Loss: 4.5426124627442945 Accumlated Loss: 4.519777140050051\n",
    "[Step 120/200] Immediate Loss: 4.526237935109501 Accumlated Loss: 4.484898105297211\n",
    "[Step 130/200] Immediate Loss: 4.473364450465856 Accumlated Loss: 4.533105885510274\n",
    "[Step 140/200] Immediate Loss: 4.567623599485331 Accumlated Loss: 4.52751955585112\n",
    "[Step 150/200] Immediate Loss: 4.41103993565831 Accumlated Loss: 4.478847055589327\n",
    "[Step 160/200] Immediate Loss: 4.511077263662673 Accumlated Loss: 4.4720754014044966\n",
    "[Step 170/200] Immediate Loss: 4.542323929173933 Accumlated Loss: 4.497481884213555\n",
    "[Step 180/200] Immediate Loss: 4.50154203534666 Accumlated Loss: 4.473875064988429\n",
    "[Step 190/200] Immediate Loss: 4.401739050099132 Accumlated Loss: 4.434624359889895\n",
    "[Step 200/200] Immediate Loss: 4.387687901801735 Accumlated Loss: 4.44847219620134"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ahead-domestic",
   "metadata": {},
   "outputs": [],
   "source": [
    "[Step 10/200] Immediate Loss: 8.831484226306204 Accumlated Loss: 92.57391344586635\n",
    "[Step 20/200] Immediate Loss: 7.672439583418599 Accumlated Loss: 8.07972444701544\n",
    "[Step 30/200] Immediate Loss: 7.132741034831854 Accumlated Loss: 7.55862317237309\n",
    "[Step 40/200] Immediate Loss: 4.677748353678033 Accumlated Loss: 5.098062943419558\n",
    "[Step 50/200] Immediate Loss: 4.679151139205498 Accumlated Loss: 4.543261697728505\n",
    "[Step 60/200] Immediate Loss: 4.584433643865686 Accumlated Loss: 4.509522491115827\n",
    "[Step 70/200] Immediate Loss: 4.435553869615634 Accumlated Loss: 4.472845446982376\n",
    "[Step 80/200] Immediate Loss: 4.473667657795313 Accumlated Loss: 4.467331585498362\n",
    "[Step 90/200] Immediate Loss: 4.57488175244379 Accumlated Loss: 4.451495232960025\n",
    "[Step 100/200] Immediate Loss: 4.411818439136213 Accumlated Loss: 4.4210633398501855\n",
    "[Step 110/200] Immediate Loss: 4.500276961633972 Accumlated Loss: 4.44278682423249\n",
    "[Step 120/200] Immediate Loss: 4.4029467117227155 Accumlated Loss: 4.448023516570168\n",
    "[Step 130/200] Immediate Loss: 4.477503563134409 Accumlated Loss: 4.449889423225052\n",
    "[Step 140/200] Immediate Loss: 4.402364695998658 Accumlated Loss: 4.421440594784628\n",
    "[Step 150/200] Immediate Loss: 4.511436905828703 Accumlated Loss: 4.419802880970236\n",
    "[Step 160/200] Immediate Loss: 4.443736040807333 Accumlated Loss: 4.431105941411857\n",
    "[Step 170/200] Immediate Loss: 4.383108533682583 Accumlated Loss: 4.39920712296829\n",
    "[Step 180/200] Immediate Loss: 4.392443198401652 Accumlated Loss: 4.42824548353719\n",
    "[Step 190/200] Immediate Loss: 4.442726319093451 Accumlated Loss: 4.4066880922388645\n",
    "[Step 200/200] Immediate Loss: 4.410327961656151 Accumlated Loss: 4.390989160202206"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dramatic-wallpaper",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 64\n",
    "[Step 10/200] Immediate Loss: 5.0620588922360925 Accumlated Loss: 27.452954740482358\n",
    "[Step 20/200] Immediate Loss: 4.713291444507197 Accumlated Loss: 4.788754089152151\n",
    "[Step 30/200] Immediate Loss: 4.634649143260541 Accumlated Loss: 4.6462725308926816\n",
    "[Step 40/200] Immediate Loss: 4.6207774791965495 Accumlated Loss: 4.6055218528176685\n",
    "[Step 50/200] Immediate Loss: 4.4578128340049785 Accumlated Loss: 4.61917436520109\n",
    "[Step 60/200] Immediate Loss: 4.447374501921998 Accumlated Loss: 4.622418769791898\n",
    "[Step 70/200] Immediate Loss: 4.543967185654741 Accumlated Loss: 4.565392534922121\n",
    "[Step 80/200] Immediate Loss: 4.578531220450167 Accumlated Loss: 4.524564780922472\n",
    "[Step 90/200] Immediate Loss: 4.405554208563901 Accumlated Loss: 4.490992267451667\n",
    "[Step 100/200] Immediate Loss: 4.446496614735619 Accumlated Loss: 4.496830442796352\n",
    "[Step 110/200] Immediate Loss: 4.547037405858325 Accumlated Loss: 4.5112183366432035\n",
    "[Step 120/200] Immediate Loss: 4.47335481010806 Accumlated Loss: 4.4820363790957485\n",
    "[Step 130/200] Immediate Loss: 4.452953545151037 Accumlated Loss: 4.513302449157598\n",
    "[Step 140/200] Immediate Loss: 4.53919651298199 Accumlated Loss: 4.512797385059419\n",
    "[Step 150/200] Immediate Loss: 4.446720929628343 Accumlated Loss: 4.458740986451288\n",
    "[Step 160/200] Immediate Loss: 4.5302174922797525 Accumlated Loss: 4.4738017470237015\n",
    "[Step 170/200] Immediate Loss: 4.535937208600031 Accumlated Loss: 4.513784821757491\n",
    "[Step 180/200] Immediate Loss: 4.49221648635508 Accumlated Loss: 4.468325105696669\n",
    "[Step 190/200] Immediate Loss: 4.401864079190618 Accumlated Loss: 4.453588182533722\n",
    "[Step 200/200] Immediate Loss: 4.396582488523782 Accumlated Loss: 4.45766376830176"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fatty-geology",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1\n",
    "[Step 10/200] Immediate Loss: 11.867968449751281 Accumlated Loss: 119.53311509676469\n",
    "[Step 20/200] Immediate Loss: 10.896902794421045 Accumlated Loss: 11.177019027288752\n",
    "[Step 30/200] Immediate Loss: 11.021964176489833 Accumlated Loss: 10.94709603698649\n",
    "[Step 40/200] Immediate Loss: 10.923975546091246 Accumlated Loss: 10.86278771985766\n",
    "[Step 50/200] Immediate Loss: 6.540438774676149 Accumlated Loss: 8.422786458936722\n",
    "[Step 60/200] Immediate Loss: 6.534668608381688 Accumlated Loss: 6.45782886935246\n",
    "[Step 70/200] Immediate Loss: 6.3713626058081 Accumlated Loss: 6.385740896648528\n",
    "[Step 80/200] Immediate Loss: 6.418236161129959 Accumlated Loss: 6.424380043813726\n",
    "[Step 90/200] Immediate Loss: 6.467850190901271 Accumlated Loss: 6.353147416662856\n",
    "[Step 100/200] Immediate Loss: 6.402001483161336 Accumlated Loss: 6.354380667378639\n",
    "[Step 110/200] Immediate Loss: 6.406023255667608 Accumlated Loss: 6.361599933337801\n",
    "[Step 120/200] Immediate Loss: 6.331160633069068 Accumlated Loss: 6.354397479556364\n",
    "[Step 130/200] Immediate Loss: 6.428838450391046 Accumlated Loss: 6.364616919689946\n",
    "[Step 140/200] Immediate Loss: 6.281965446018525 Accumlated Loss: 6.350332170550652\n",
    "[Step 150/200] Immediate Loss: 6.487718800097312 Accumlated Loss: 6.357617957601458\n",
    "[Step 160/200] Immediate Loss: 6.420994718482842 Accumlated Loss: 6.356143715201076\n",
    "[Step 170/200] Immediate Loss: 6.366521451198219 Accumlated Loss: 6.355294009417903\n",
    "[Step 180/200] Immediate Loss: 6.330180337278418 Accumlated Loss: 6.355438968158276\n",
    "[Step 190/200] Immediate Loss: 6.351852865357609 Accumlated Loss: 6.327770651926091\n",
    "[Step 200/200] Immediate Loss: 6.335215225769062 Accumlated Loss: 6.30661983309009"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lesser-genome",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 \n",
    "[Step 10/200] Immediate Loss: 9.346970654129983 Accumlated Loss: 113.1739841233491\n",
    "[Step 20/200] Immediate Loss: 6.975127419019119 Accumlated Loss: 7.69983969870489\n",
    "[Step 30/200] Immediate Loss: 6.743622715440579 Accumlated Loss: 6.692427675213664\n",
    "[Step 40/200] Immediate Loss: 6.436417305622891 Accumlated Loss: 6.471210175176849\n",
    "[Step 50/200] Immediate Loss: 6.713871218829665 Accumlated Loss: 6.458653660540082\n",
    "[Step 60/200] Immediate Loss: 6.484426397361198 Accumlated Loss: 6.472620006053537\n",
    "[Step 70/200] Immediate Loss: 6.405989019334527 Accumlated Loss: 6.412187910313097\n",
    "[Step 80/200] Immediate Loss: 6.362180096338657 Accumlated Loss: 6.401051664661315\n",
    "[Step 90/200] Immediate Loss: 6.4202219687911635 Accumlated Loss: 6.375913588481626\n",
    "[Step 100/200] Immediate Loss: 6.364927740305324 Accumlated Loss: 6.360376990470423\n",
    "[Step 110/200] Immediate Loss: 6.421359613376986 Accumlated Loss: 6.377258479340531\n",
    "[Step 120/200] Immediate Loss: 6.318274569223457 Accumlated Loss: 6.363953679904944\n",
    "[Step 130/200] Immediate Loss: 6.438532743051219 Accumlated Loss: 6.370663608934299\n",
    "[Step 140/200] Immediate Loss: 6.2974191956826395 Accumlated Loss: 6.370326233826152\n",
    "[Step 150/200] Immediate Loss: 6.5141160295399185 Accumlated Loss: 6.3641636528517935\n",
    "[Step 160/200] Immediate Loss: 6.415865887324406 Accumlated Loss: 6.362849791377189\n",
    "[Step 170/200] Immediate Loss: 6.376131625174 Accumlated Loss: 6.371652579213016\n",
    "[Step 180/200] Immediate Loss: 6.3248094469067215 Accumlated Loss: 6.36185570209\n",
    "[Step 190/200] Immediate Loss: 6.433083155437597 Accumlated Loss: 6.338922789049341\n",
    "[Step 200/200] Immediate Loss: 6.318668476126111 Accumlated Loss: 6.312395074093867"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "curious-snowboard",
   "metadata": {},
   "outputs": [],
   "source": [
    "[Step 10/200] Immediate Loss: 11.976727526047911 Accumlated Loss: 119.53362743361764\n",
    "[Step 20/200] Immediate Loss: 10.916181364493935 Accumlated Loss: 11.219259248388454\n",
    "[Step 30/200] Immediate Loss: 11.043343579947303 Accumlated Loss: 10.973555087051766\n",
    "[Step 40/200] Immediate Loss: 10.943668668569366 Accumlated Loss: 10.884751558892942\n",
    "[Step 50/200] Immediate Loss: 6.559941983489846 Accumlated Loss: 8.770927740397756\n",
    "[Step 60/200] Immediate Loss: 6.5468337412347655 Accumlated Loss: 6.469271936579125\n",
    "[Step 70/200] Immediate Loss: 6.375677891010669 Accumlated Loss: 6.388119940882053\n",
    "[Step 80/200] Immediate Loss: 6.4188883962476675 Accumlated Loss: 6.392748077415217\n",
    "[Step 90/200] Immediate Loss: 6.455901776794543 Accumlated Loss: 6.356703812085604\n",
    "[Step 100/200] Immediate Loss: 6.369250865431012 Accumlated Loss: 6.352617141379314\n",
    "[Step 110/200] Immediate Loss: 6.420737668370759 Accumlated Loss: 6.367711289280597\n",
    "[Step 120/200] Immediate Loss: 6.352763859998925 Accumlated Loss: 6.354864450324396\n",
    "[Step 130/200] Immediate Loss: 6.516434524765906 Accumlated Loss: 6.371440517413238\n",
    "[Step 140/200] Immediate Loss: 6.317252023398081 Accumlated Loss: 6.360252985506631\n",
    "[Step 150/200] Immediate Loss: 6.513535037960472 Accumlated Loss: 6.360937603042849\n",
    "[Step 160/200] Immediate Loss: 6.415885079816072 Accumlated Loss: 6.360746409138264\n",
    "[Step 170/200] Immediate Loss: 6.36712605305134 Accumlated Loss: 6.351436705747479\n",
    "[Step 180/200] Immediate Loss: 6.3286450622526305 Accumlated Loss: 6.354895116672755\n",
    "[Step 190/200] Immediate Loss: 6.341010142530811 Accumlated Loss: 6.330026590914046\n",
    "[Step 200/200] Immediate Loss: 6.329053945091422 Accumlated Loss: 6.301397666465279"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "monetary-helen",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "signal-father",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "designed-sixth",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dressed-athens",
   "metadata": {},
   "outputs": [],
   "source": [
    "[Step 10/200] Immediate Loss: 21.522718148746375 Accumlated Loss: 207.41512029347075\n",
    "[Step 20/200] Immediate Loss: 21.41585597807821 Accumlated Loss: 21.62552405459855\n",
    "[Step 30/200] Immediate Loss: 21.38234384285553 Accumlated Loss: 21.344668118501385\n",
    "[Step 40/200] Immediate Loss: 21.313893368692646 Accumlated Loss: 21.253254188452377\n",
    "[Step 50/200] Immediate Loss: 21.363491764812153 Accumlated Loss: 21.303800385953817\n",
    "[Step 60/200] Immediate Loss: 21.306386903702688 Accumlated Loss: 21.349030146154583\n",
    "[Step 70/200] Immediate Loss: 21.3736292214927 Accumlated Loss: 21.306491545348806\n",
    "[Step 80/200] Immediate Loss: 21.421294747908846 Accumlated Loss: 21.33118316067945\n",
    "[Step 90/200] Immediate Loss: 13.87480960085825 Accumlated Loss: 17.12417806620567\n",
    "[Step 100/200] Immediate Loss: 13.927702356373555 Accumlated Loss: 13.906270983097054\n",
    "[Step 110/200] Immediate Loss: 14.007013700691347 Accumlated Loss: 13.887582020328281\n",
    "[Step 120/200] Immediate Loss: 13.89133027452584 Accumlated Loss: 13.894257074591739\n",
    "[Step 130/200] Immediate Loss: 13.889141324461221 Accumlated Loss: 13.89091330057241\n",
    "[Step 140/200] Immediate Loss: 13.84332546837301 Accumlated Loss: 13.889453015338978\n",
    "[Step 150/200] Immediate Loss: 13.888112702304891 Accumlated Loss: 13.878427546747785\n",
    "[Step 160/200] Immediate Loss: 13.94087721885077 Accumlated Loss: 13.909350519925027\n",
    "[Step 170/200] Immediate Loss: 13.962335496115275 Accumlated Loss: 13.914065014255666\n",
    "[Step 180/200] Immediate Loss: 14.022486991893508 Accumlated Loss: 13.890356543407954\n",
    "[Step 190/200] Immediate Loss: 13.875961518199109 Accumlated Loss: 13.86669538253719\n",
    "[Step 200/200] Immediate Loss: 13.785965126088437 Accumlated Loss: 13.827820913653575"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uniform-essay",
   "metadata": {},
   "outputs": [],
   "source": [
    "[Step 10/200] Immediate Loss: 21.536989645789088 Accumlated Loss: 207.38362745398382\n",
    "[Step 20/200] Immediate Loss: 21.431367387868757 Accumlated Loss: 21.642072230243198\n",
    "[Step 30/200] Immediate Loss: 21.396480620934998 Accumlated Loss: 21.355699536390183\n",
    "[Step 40/200] Immediate Loss: 21.317475201955222 Accumlated Loss: 21.257319459231077\n",
    "[Step 50/200] Immediate Loss: 21.388170568096967 Accumlated Loss: 21.30689317368248\n",
    "[Step 60/200] Immediate Loss: 21.29509833189178 Accumlated Loss: 21.37378634752352\n",
    "[Step 70/200] Immediate Loss: 21.39207734857011 Accumlated Loss: 21.309895196958443\n",
    "[Step 80/200] Immediate Loss: 21.41701041480391 Accumlated Loss: 21.34648537312752\n",
    "[Step 90/200] Immediate Loss: 13.900549236834985 Accumlated Loss: 18.788400901073675\n",
    "[Step 100/200] Immediate Loss: 13.946606806151816 Accumlated Loss: 13.927764978277349\n",
    "[Step 110/200] Immediate Loss: 13.93928362660724 Accumlated Loss: 13.86846119425471\n",
    "[Step 120/200] Immediate Loss: 13.879952646393416 Accumlated Loss: 13.887025538922336\n",
    "[Step 130/200] Immediate Loss: 13.859915165703011 Accumlated Loss: 13.89090403190948\n",
    "[Step 140/200] Immediate Loss: 13.843595463325856 Accumlated Loss: 13.907684028480258\n",
    "[Step 150/200] Immediate Loss: 13.987846254238427 Accumlated Loss: 13.876980365017154\n",
    "[Step 160/200] Immediate Loss: 13.942431985275736 Accumlated Loss: 13.889264042637969\n",
    "[Step 170/200] Immediate Loss: 13.959955432193178 Accumlated Loss: 13.911328180621643\n",
    "[Step 180/200] Immediate Loss: 13.879409624821916 Accumlated Loss: 13.861598426539615\n",
    "[Step 190/200] Immediate Loss: 13.820956494832117 Accumlated Loss: 13.851386304608157\n",
    "[Step 200/200] Immediate Loss: 13.781572264708917 Accumlated Loss: 13.902101758820784"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "agreed-shakespeare",
   "metadata": {},
   "outputs": [],
   "source": [
    "[Step 10/200] Immediate Loss: 19.228435556888595 Accumlated Loss: 200.26897022835902\n",
    "[Step 20/200] Immediate Loss: 14.617401484767901 Accumlated Loss: 16.215209771615914\n",
    "[Step 30/200] Immediate Loss: 14.258991522621649 Accumlated Loss: 14.351622893503107\n",
    "[Step 40/200] Immediate Loss: 13.953358450610619 Accumlated Loss: 13.975740830798694\n",
    "[Step 50/200] Immediate Loss: 13.966162201228464 Accumlated Loss: 13.958601482277473\n",
    "[Step 60/200] Immediate Loss: 13.895017380411408 Accumlated Loss: 14.04609221756147\n",
    "[Step 70/200] Immediate Loss: 14.049807088218627 Accumlated Loss: 14.019568621321302\n",
    "[Step 80/200] Immediate Loss: 13.96307481053616 Accumlated Loss: 13.975776579574655\n",
    "[Step 90/200] Immediate Loss: 13.941820238092808 Accumlated Loss: 13.92562076001628\n",
    "[Step 100/200] Immediate Loss: 13.981783624634213 Accumlated Loss: 13.921046124255986\n",
    "[Step 110/200] Immediate Loss: 13.985471663292016 Accumlated Loss: 13.920347134704944\n",
    "[Step 120/200] Immediate Loss: 13.892946001580935 Accumlated Loss: 13.91870265561967\n",
    "[Step 130/200] Immediate Loss: 13.866418977553318 Accumlated Loss: 14.143524776566458\n",
    "[Step 140/200] Immediate Loss: 13.881403735286325 Accumlated Loss: 13.907811260371544\n",
    "[Step 150/200] Immediate Loss: 13.889705268616018 Accumlated Loss: 13.897106436001556\n",
    "[Step 160/200] Immediate Loss: 13.98417887743753 Accumlated Loss: 13.92414206849322\n",
    "[Step 170/200] Immediate Loss: 13.977509658064132 Accumlated Loss: 13.913636127557293\n",
    "[Step 180/200] Immediate Loss: 14.01252704814085 Accumlated Loss: 13.928915505240232\n",
    "[Step 190/200] Immediate Loss: 13.868995597070706 Accumlated Loss: 13.867599604848294\n",
    "[Step 200/200] Immediate Loss: 13.802424267921582 Accumlated Loss: 13.839087068237767"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "previous-letters",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "transsexual-remains",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 234\n",
    "[Step 10/200] Immediate Loss: 7.195491719357669 Accumlated Loss: 39.700878605598575\n",
    "[Step 20/200] Immediate Loss: 5.6998961744457475 Accumlated Loss: 6.26671837088093\n",
    "[Step 30/200] Immediate Loss: 4.692294080862777 Accumlated Loss: 4.946705584792887\n",
    "[Step 40/200] Immediate Loss: 4.796591249112097 Accumlated Loss: 4.752417349870171\n",
    "[Step 50/200] Immediate Loss: 4.621495101320762 Accumlated Loss: 4.670041528392663\n",
    "[Step 60/200] Immediate Loss: 4.534939376313593 Accumlated Loss: 4.618246412111732\n",
    "[Step 70/200] Immediate Loss: 4.585024912086808 Accumlated Loss: 4.551269153323987\n",
    "[Step 80/200] Immediate Loss: 4.454118347572948 Accumlated Loss: 4.474291823778582\n",
    "[Step 90/200] Immediate Loss: 4.4294447009708335 Accumlated Loss: 4.472697340578821\n",
    "[Step 100/200] Immediate Loss: 4.429915012706409 Accumlated Loss: 4.477198233705097\n",
    "[Step 110/200] Immediate Loss: 4.5613434716909556 Accumlated Loss: 4.455384294834135\n",
    "[Step 120/200] Immediate Loss: 4.443874849736525 Accumlated Loss: 4.425778731042524\n",
    "[Step 130/200] Immediate Loss: 4.571623991941054 Accumlated Loss: 4.450863315437208\n",
    "[Step 140/200] Immediate Loss: 4.510091479033561 Accumlated Loss: 4.427214763087768\n",
    "[Step 150/200] Immediate Loss: 4.523874952274341 Accumlated Loss: 4.427721300816889\n",
    "[Step 160/200] Immediate Loss: 4.454986119801448 Accumlated Loss: 4.432565965245174\n",
    "[Step 170/200] Immediate Loss: 4.372630771429708 Accumlated Loss: 4.390093631051786\n",
    "[Step 180/200] Immediate Loss: 4.448110207207775 Accumlated Loss: 4.423109177469071\n",
    "[Step 190/200] Immediate Loss: 4.415633551918446 Accumlated Loss: 4.4107278995792125\n",
    "[Step 200/200] Immediate Loss: 4.496516553735528 Accumlated Loss: 4.427401870993286"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prompt-session",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 234\n",
    "[Step 10/200] Immediate Loss: 5.163286211934024 Accumlated Loss: 89.67468019718086\n",
    "[Step 20/200] Immediate Loss: 4.85941919376055 Accumlated Loss: 5.134031581087738\n",
    "[Step 30/200] Immediate Loss: 4.585264132897757 Accumlated Loss: 4.708425559706526\n",
    "[Step 40/200] Immediate Loss: 4.679668403368051 Accumlated Loss: 4.586337144322442\n",
    "[Step 50/200] Immediate Loss: 4.5222279742961105 Accumlated Loss: 4.528223784671318\n",
    "[Step 60/200] Immediate Loss: 4.481008359609841 Accumlated Loss: 4.513519197768703\n",
    "[Step 70/200] Immediate Loss: 4.473330731441311 Accumlated Loss: 4.449387745317834\n",
    "[Step 80/200] Immediate Loss: 4.449380338746997 Accumlated Loss: 4.430350501309596\n",
    "[Step 90/200] Immediate Loss: 4.417518493975252 Accumlated Loss: 4.453708892139199\n",
    "[Step 100/200] Immediate Loss: 4.413964221336027 Accumlated Loss: 4.450385621989551\n",
    "[Step 110/200] Immediate Loss: 4.564237365521452 Accumlated Loss: 4.451846125569707\n",
    "[Step 120/200] Immediate Loss: 4.442553746158893 Accumlated Loss: 4.424714446612678\n",
    "[Step 130/200] Immediate Loss: 4.518962269911807 Accumlated Loss: 4.443732491511808\n",
    "[Step 140/200] Immediate Loss: 4.545073836554788 Accumlated Loss: 4.440178459096454\n",
    "[Step 150/200] Immediate Loss: 4.470176245754756 Accumlated Loss: 4.434489087089757\n",
    "[Step 160/200] Immediate Loss: 4.422263964162077 Accumlated Loss: 4.40676958991898\n",
    "[Step 170/200] Immediate Loss: 4.383147394212428 Accumlated Loss: 4.4095130517454315\n",
    "[Step 180/200] Immediate Loss: 4.429771131227608 Accumlated Loss: 4.606229147210775\n",
    "[Step 190/200] Immediate Loss: 4.3953610116103645 Accumlated Loss: 4.414204198712615\n",
    "[Step 200/200] Immediate Loss: 4.4102370617644935 Accumlated Loss: 4.444824702411117"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
